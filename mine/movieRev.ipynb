{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vagrant/data/mine\n",
      "['review_polarity.tar.gz']\n"
     ]
    }
   ],
   "source": [
    "#import tarfile\n",
    "#import os\n",
    "#os.chdir('/home/vagrant/data/mine')\n",
    "#print os.getcwd()\n",
    "\n",
    "#print os.listdir(os.getcwd())\n",
    "#fh = tarfile.open('review_polarity.tar.gz', 'r:gz')\n",
    "\n",
    "#fh.extractall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negdata=sc.wholeTextFiles('/home/vagrant/data/mine/txt_sentoken/neg/').map(lambda a:(1 if 'pos' in a[0] else 0,a[1]))\n",
    "posdata=sc.wholeTextFiles('/home/vagrant/data/mine/txt_sentoken/pos/').map(lambda a:(1 if 'pos' in a[0] else 0,a[1]))\n",
    "#posdata=sc.union([sc.textFile('/home/vagrant/data/mine/txt_sentoken/pos/' + \"\\n\" + f) for f in os.listdir('/home/vagrant/data/mine/txt_sentoken/pos/')])\n",
    "#posdata=sc.wholeTextFiles('/home/vagrant/data/mine/txt_sentoken/pos/'+os.listdir('/home/vagrant/data/mine/txt_sentoken/pos/')[0])\n",
    "data=negdata.union(posdata)\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "data=data.cache()\n",
    "negdata=negdata.cache()\n",
    "posdata=posdata.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "documents=data.map(lambda a:a[1].split(' '))\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'the',\n",
       "  u'first',\n",
       "  u'image',\n",
       "  u'in',\n",
       "  u'\"',\n",
       "  u'final',\n",
       "  u'fantasy',\n",
       "  u':',\n",
       "  u'the',\n",
       "  u'spirits',\n",
       "  u'within',\n",
       "  u'\"',\n",
       "  u'is',\n",
       "  u'a',\n",
       "  u'computer-animated',\n",
       "  u'close-up',\n",
       "  u'of',\n",
       "  u'a',\n",
       "  u'human',\n",
       "  u'eye',\n",
       "  u'.',\n",
       "  u\"\\nit's\",\n",
       "  u'a',\n",
       "  u'beautiful',\n",
       "  u'piece',\n",
       "  u'of',\n",
       "  u'work',\n",
       "  u',',\n",
       "  u'remarkably',\n",
       "  u'detailed',\n",
       "  u'and',\n",
       "  u'quite',\n",
       "  u'convincing',\n",
       "  u'.',\n",
       "  u'\\nwhen',\n",
       "  u'the',\n",
       "  u'picture',\n",
       "  u'pulls',\n",
       "  u'back',\n",
       "  u'to',\n",
       "  u'reveal',\n",
       "  u'the',\n",
       "  u'owner',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'eye',\n",
       "  u',',\n",
       "  u'however',\n",
       "  u',',\n",
       "  u'things',\n",
       "  u'change',\n",
       "  u'.',\n",
       "  u'\\nthe',\n",
       "  u'young',\n",
       "  u'woman',\n",
       "  u'has',\n",
       "  u'mesmerizing',\n",
       "  u'hair',\n",
       "  u',',\n",
       "  u'although',\n",
       "  u'it',\n",
       "  u'hangs',\n",
       "  u'too',\n",
       "  u'artfully',\n",
       "  u'?',\n",
       "  u'even',\n",
       "  u'by',\n",
       "  u'movie',\n",
       "  u'standards',\n",
       "  u'?',\n",
       "  u'to',\n",
       "  u'be',\n",
       "  u'believed',\n",
       "  u'.',\n",
       "  u'\\nthe',\n",
       "  u'facial',\n",
       "  u'features',\n",
       "  u'are',\n",
       "  u'more',\n",
       "  u'detailed',\n",
       "  u'than',\n",
       "  u'any',\n",
       "  u'computer-animation',\n",
       "  u'seen',\n",
       "  u'to',\n",
       "  u'date',\n",
       "  u',',\n",
       "  u'but',\n",
       "  u'the',\n",
       "  u'result',\n",
       "  u'is',\n",
       "  u'more',\n",
       "  u'reminiscent',\n",
       "  u'of',\n",
       "  u'a',\n",
       "  u'very',\n",
       "  u'well-crafted',\n",
       "  u'doll',\n",
       "  u'than',\n",
       "  u'anything',\n",
       "  u'human',\n",
       "  u'.',\n",
       "  u'\\nshe',\n",
       "  u'is',\n",
       "  u'pretty',\n",
       "  u',',\n",
       "  u'but',\n",
       "  u'bland',\n",
       "  u',',\n",
       "  u'and',\n",
       "  u'not',\n",
       "  u'nearly',\n",
       "  u'expressive',\n",
       "  u'enough',\n",
       "  u'to',\n",
       "  u'come',\n",
       "  u'off',\n",
       "  u'like',\n",
       "  u'a',\n",
       "  u'person',\n",
       "  u'.',\n",
       "  u'\\nall',\n",
       "  u'the',\n",
       "  u'characters',\n",
       "  u'in',\n",
       "  u'\"',\n",
       "  u'final',\n",
       "  u'fantasy',\n",
       "  u'\"',\n",
       "  u'are',\n",
       "  u'like',\n",
       "  u'that',\n",
       "  u'.',\n",
       "  u'\\nof',\n",
       "  u'the',\n",
       "  u'core',\n",
       "  u'group',\n",
       "  u',',\n",
       "  u'the',\n",
       "  u'younger',\n",
       "  u'white',\n",
       "  u'men',\n",
       "  u'and',\n",
       "  u'women',\n",
       "  u'are',\n",
       "  u'all',\n",
       "  u'athletic',\n",
       "  u',',\n",
       "  u'attractive',\n",
       "  u'and',\n",
       "  u'indistinct',\n",
       "  u',',\n",
       "  u'like',\n",
       "  u'applicants',\n",
       "  u'for',\n",
       "  u'a',\n",
       "  u'tv',\n",
       "  u'reality',\n",
       "  u'show',\n",
       "  u'.',\n",
       "  u'\\nthe',\n",
       "  u'black',\n",
       "  u'man',\n",
       "  u'is',\n",
       "  u'taller',\n",
       "  u'and',\n",
       "  u'burlier',\n",
       "  u',',\n",
       "  u'and',\n",
       "  u'the',\n",
       "  u'aging',\n",
       "  u'scholar',\n",
       "  u'is',\n",
       "  u'bald',\n",
       "  u',',\n",
       "  u'with',\n",
       "  u'wrinkles',\n",
       "  u'and',\n",
       "  u'a',\n",
       "  u'beard',\n",
       "  u'.',\n",
       "  u'\\nnone',\n",
       "  u'of',\n",
       "  u'them',\n",
       "  u'appear',\n",
       "  u'to',\n",
       "  u'be',\n",
       "  u'based',\n",
       "  u'on',\n",
       "  u'individuals',\n",
       "  u';',\n",
       "  u'they',\n",
       "  u'all',\n",
       "  u'look',\n",
       "  u'like',\n",
       "  u'the',\n",
       "  u'products',\n",
       "  u'of',\n",
       "  u'general',\n",
       "  u'descriptions',\n",
       "  u'given',\n",
       "  u'a',\n",
       "  u'police',\n",
       "  u'sketch',\n",
       "  u'artist',\n",
       "  u'.',\n",
       "  u'\\nit',\n",
       "  u'gets',\n",
       "  u'worse',\n",
       "  u'when',\n",
       "  u'they',\n",
       "  u'talk',\n",
       "  u'and',\n",
       "  u'move',\n",
       "  u'.',\n",
       "  u'\\nwhy',\n",
       "  u'is',\n",
       "  u'the',\n",
       "  u'sarcastic',\n",
       "  u'voice',\n",
       "  u'of',\n",
       "  u'steve',\n",
       "  u'buscemi',\n",
       "  u',',\n",
       "  u'he',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'great',\n",
       "  u'twisted',\n",
       "  u'face',\n",
       "  u'and',\n",
       "  u'snaggleteeth',\n",
       "  u',',\n",
       "  u'coming',\n",
       "  u'out',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'mouth',\n",
       "  u'of',\n",
       "  u'some',\n",
       "  u'dreary',\n",
       "  u'ken',\n",
       "  u'doll',\n",
       "  u'?',\n",
       "  u'\\nwhy',\n",
       "  u',',\n",
       "  u'for',\n",
       "  u'every',\n",
       "  u'fluid',\n",
       "  u'physical',\n",
       "  u'gesture',\n",
       "  u',',\n",
       "  u'do',\n",
       "  u'we',\n",
       "  u'also',\n",
       "  u'see',\n",
       "  u'herky-jerky',\n",
       "  u'puppet-style',\n",
       "  u'motions',\n",
       "  u'?',\n",
       "  u'\\nmore',\n",
       "  u'to',\n",
       "  u'the',\n",
       "  u'point',\n",
       "  u',',\n",
       "  u'who',\n",
       "  u'decided',\n",
       "  u'a',\n",
       "  u'full-length',\n",
       "  u'computer',\n",
       "  u'animated',\n",
       "  u'movie',\n",
       "  u'featuring',\n",
       "  u'\"',\n",
       "  u'hyperreal',\n",
       "  u'\"',\n",
       "  u'(',\n",
       "  u'their',\n",
       "  u'term',\n",
       "  u',',\n",
       "  u'not',\n",
       "  u'mine',\n",
       "  u')',\n",
       "  u'humanoids',\n",
       "  u'was',\n",
       "  u'a',\n",
       "  u'good',\n",
       "  u'idea',\n",
       "  u'?',\n",
       "  u'\\n',\n",
       "  u'\"',\n",
       "  u'final',\n",
       "  u'fantasy',\n",
       "  u'\"',\n",
       "  u'is',\n",
       "  u'based',\n",
       "  u'on',\n",
       "  u'a',\n",
       "  u'phenomenally',\n",
       "  u'popular',\n",
       "  u'video',\n",
       "  u'game',\n",
       "  u\"i've\",\n",
       "  u'never',\n",
       "  u'played',\n",
       "  u',',\n",
       "  u'with',\n",
       "  u'a',\n",
       "  u'story',\n",
       "  u'straight',\n",
       "  u'out',\n",
       "  u'of',\n",
       "  u'japanese',\n",
       "  u'anime',\n",
       "  u',',\n",
       "  u'which',\n",
       "  u'more',\n",
       "  u'often',\n",
       "  u'than',\n",
       "  u'not',\n",
       "  u'leaves',\n",
       "  u'me',\n",
       "  u'bored',\n",
       "  u'and',\n",
       "  u'depressed',\n",
       "  u'.',\n",
       "  u'\\nif',\n",
       "  u\"you're\",\n",
       "  u'a',\n",
       "  u'fan',\n",
       "  u'of',\n",
       "  u'either',\n",
       "  u',',\n",
       "  u'please',\n",
       "  u'spare',\n",
       "  u'me',\n",
       "  u'your',\n",
       "  u'letters',\n",
       "  u',',\n",
       "  u'as',\n",
       "  u'i',\n",
       "  u'will',\n",
       "  u'focus',\n",
       "  u'solely',\n",
       "  u'on',\n",
       "  u'the',\n",
       "  u'finished',\n",
       "  u'film',\n",
       "  u'and',\n",
       "  u'not',\n",
       "  u'its',\n",
       "  u'source',\n",
       "  u'materials',\n",
       "  u'.',\n",
       "  u'\\nwith',\n",
       "  u'an',\n",
       "  u'expression-challenged',\n",
       "  u'cast',\n",
       "  u',',\n",
       "  u'\"',\n",
       "  u'final',\n",
       "  u'fantasy',\n",
       "  u'\"',\n",
       "  u'mixes',\n",
       "  u'turgid',\n",
       "  u'action',\n",
       "  u'scenes',\n",
       "  u'with',\n",
       "  u'heaps',\n",
       "  u'of',\n",
       "  u'mystical',\n",
       "  u'shit',\n",
       "  u'.',\n",
       "  u'\\nthe',\n",
       "  u'result',\n",
       "  u'is',\n",
       "  u'ugly',\n",
       "  u',',\n",
       "  u'confusing',\n",
       "  u'and',\n",
       "  u'boring',\n",
       "  u'.',\n",
       "  u'\\nnote',\n",
       "  u':',\n",
       "  u'the',\n",
       "  u'following',\n",
       "  u'reveals',\n",
       "  u'the',\n",
       "  u'basic',\n",
       "  u'plot',\n",
       "  u'.',\n",
       "  u'\\nif',\n",
       "  u'you',\n",
       "  u'want',\n",
       "  u'to',\n",
       "  u'have',\n",
       "  u'a',\n",
       "  u'fighting',\n",
       "  u'chance',\n",
       "  u'of',\n",
       "  u'making',\n",
       "  u'any',\n",
       "  u'sense',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'movie',\n",
       "  u',',\n",
       "  u'i',\n",
       "  u'suggest',\n",
       "  u'you',\n",
       "  u'read',\n",
       "  u'it',\n",
       "  u'.',\n",
       "  u'\\nearth',\n",
       "  u'is',\n",
       "  u'at',\n",
       "  u'war',\n",
       "  u'with',\n",
       "  u'aliens',\n",
       "  u'that',\n",
       "  u'appear',\n",
       "  u'to',\n",
       "  u'feed',\n",
       "  u'on',\n",
       "  u'human',\n",
       "  u'souls',\n",
       "  u'.',\n",
       "  u'\\nmost',\n",
       "  u'of',\n",
       "  u'our',\n",
       "  u'planet',\n",
       "  u'is',\n",
       "  u'devastated',\n",
       "  u',',\n",
       "  u'with',\n",
       "  u'humans',\n",
       "  u'living',\n",
       "  u'in',\n",
       "  u'a',\n",
       "  u'few',\n",
       "  u'protected',\n",
       "  u'cities',\n",
       "  u'.',\n",
       "  u'\\nwhile',\n",
       "  u'the',\n",
       "  u'bulk',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'survivors',\n",
       "  u'focus',\n",
       "  u'on',\n",
       "  u'military',\n",
       "  u'strategies',\n",
       "  u',',\n",
       "  u'aki',\n",
       "  u'ross',\n",
       "  u'(',\n",
       "  u'voiced',\n",
       "  u'by',\n",
       "  u'ming-na',\n",
       "  u')',\n",
       "  u'and',\n",
       "  u'her',\n",
       "  u'mentor',\n",
       "  u',',\n",
       "  u'dr',\n",
       "  u'.',\n",
       "  u'sid',\n",
       "  u'(',\n",
       "  u'donald',\n",
       "  u'sutherland',\n",
       "  u')',\n",
       "  u',',\n",
       "  u'believe',\n",
       "  u'in',\n",
       "  u'a',\n",
       "  u'more',\n",
       "  u'organic',\n",
       "  u'approach',\n",
       "  u'.',\n",
       "  u'\\nthey',\n",
       "  u'operate',\n",
       "  u'on',\n",
       "  u'the',\n",
       "  u'notion',\n",
       "  u'(',\n",
       "  u'quoting',\n",
       "  u'straight',\n",
       "  u'from',\n",
       "  u'the',\n",
       "  u'press',\n",
       "  u'kit',\n",
       "  u')',\n",
       "  u'\"',\n",
       "  u'that',\n",
       "  u'all',\n",
       "  u'life',\n",
       "  u'forms',\n",
       "  u'have',\n",
       "  u'signature',\n",
       "  u'spirit',\n",
       "  u'waves',\n",
       "  u'that',\n",
       "  u'can',\n",
       "  u'be',\n",
       "  u'identified',\n",
       "  u'and',\n",
       "  u'contained',\n",
       "  u'.',\n",
       "  u'\\naki',\n",
       "  u'and',\n",
       "  u'dr',\n",
       "  u'.',\n",
       "  u'sid',\n",
       "  u'collect',\n",
       "  u'a',\n",
       "  u'series',\n",
       "  u'of',\n",
       "  u'organic',\n",
       "  u'specimens',\n",
       "  u'whose',\n",
       "  u'spirit',\n",
       "  u'signatures',\n",
       "  u'combined',\n",
       "  u'will',\n",
       "  u'form',\n",
       "  u'a',\n",
       "  u'wave',\n",
       "  u'of',\n",
       "  u'equal',\n",
       "  u'and',\n",
       "  u'opposite',\n",
       "  u'intensity',\n",
       "  u'to',\n",
       "  u'the',\n",
       "  u'spirit',\n",
       "  u'wave',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'alien',\n",
       "  u'force',\n",
       "  u'.',\n",
       "  u'\\nthe',\n",
       "  u'waves',\n",
       "  u'will',\n",
       "  u',',\n",
       "  u'in',\n",
       "  u'effect',\n",
       "  u',',\n",
       "  u'cancel',\n",
       "  u'each',\n",
       "  u'other',\n",
       "  u'out',\n",
       "  u'and',\n",
       "  u'disarm',\n",
       "  u'the',\n",
       "  u'foreign',\n",
       "  u'contagion',\n",
       "  u'.',\n",
       "  u'\\nthey',\n",
       "  u'have',\n",
       "  u'collected',\n",
       "  u'six',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'eight',\n",
       "  u'key',\n",
       "  u'spirits',\n",
       "  u'needed',\n",
       "  u'to',\n",
       "  u'complete',\n",
       "  u'their',\n",
       "  u'wave',\n",
       "  u'.',\n",
       "  u'\\nthey',\n",
       "  u'are',\n",
       "  u'on',\n",
       "  u'a',\n",
       "  u'desperate',\n",
       "  u'hunt',\n",
       "  u'to',\n",
       "  u'find',\n",
       "  u'the',\n",
       "  u'remaining',\n",
       "  u'two',\n",
       "  u'spirits',\n",
       "  u'before',\n",
       "  u'their',\n",
       "  u'time',\n",
       "  u'runs',\n",
       "  u'out',\n",
       "  u'.',\n",
       "  u'\"',\n",
       "  u'\\nare',\n",
       "  u'you',\n",
       "  u'still',\n",
       "  u'with',\n",
       "  u'me',\n",
       "  u'?',\n",
       "  u\"\\nthere's\",\n",
       "  u'only',\n",
       "  u'a',\n",
       "  u'little',\n",
       "  u'more',\n",
       "  u'.',\n",
       "  u'\\naki',\n",
       "  u'is',\n",
       "  u'infected',\n",
       "  u'with',\n",
       "  u'the',\n",
       "  u'alien',\n",
       "  u'force',\n",
       "  u'.',\n",
       "  u'\\ndr',\n",
       "  u'.',\n",
       "  u'sid',\n",
       "  u'has',\n",
       "  u'developed',\n",
       "  u'a',\n",
       "  u'method',\n",
       "  u'of',\n",
       "  u'confining',\n",
       "  u'the',\n",
       "  u'contagion',\n",
       "  u'and',\n",
       "  u'keeping',\n",
       "  u'it',\n",
       "  u'from',\n",
       "  u'killing',\n",
       "  u'her',\n",
       "  u',',\n",
       "  u'but',\n",
       "  u'the',\n",
       "  u'defense',\n",
       "  u'wall',\n",
       "  u\"won't\",\n",
       "  u'hold',\n",
       "  u'much',\n",
       "  u'longer',\n",
       "  u'.',\n",
       "  u'\\nalready',\n",
       "  u',',\n",
       "  u'the',\n",
       "  u'alien',\n",
       "  u'is',\n",
       "  u'communicating',\n",
       "  u'with',\n",
       "  u'aki',\n",
       "  u'through',\n",
       "  u'her',\n",
       "  u'dreams',\n",
       "  u'.',\n",
       "  u'\\naiding',\n",
       "  u'aki',\n",
       "  u'and',\n",
       "  u'dr',\n",
       "  u'.',\n",
       "  u'sid',\n",
       "  u'are',\n",
       "  u'the',\n",
       "  u'deep',\n",
       "  u'eyes',\n",
       "  u',',\n",
       "  u'a',\n",
       "  u'group',\n",
       "  u'of',\n",
       "  u'hard-as-nails',\n",
       "  u'types',\n",
       "  u'that',\n",
       "  u'would',\n",
       "  u'have',\n",
       "  u'felt',\n",
       "  u'at',\n",
       "  u'home',\n",
       "  u'with',\n",
       "  u'the',\n",
       "  u'troops',\n",
       "  u'in',\n",
       "  u'\"',\n",
       "  u'aliens',\n",
       "  u'.',\n",
       "  u'\"',\n",
       "  u'\\ncapt',\n",
       "  u'.',\n",
       "  u'gray',\n",
       "  u'edwards',\n",
       "  u'(',\n",
       "  u'alec',\n",
       "  u'baldwin',\n",
       "  u')',\n",
       "  u'heads',\n",
       "  u'the',\n",
       "  u'task',\n",
       "  u'force',\n",
       "  u'that',\n",
       "  u'consists',\n",
       "  u'of',\n",
       "  u'a',\n",
       "  u'wise',\n",
       "  u'guy',\n",
       "  u'(',\n",
       "  u'buscemi',\n",
       "  u')',\n",
       "  u',',\n",
       "  u'a',\n",
       "  u'tough',\n",
       "  u'woman',\n",
       "  u'(',\n",
       "  u'peri',\n",
       "  u'gilpin',\n",
       "  u')',\n",
       "  u'and',\n",
       "  u'a',\n",
       "  u'gentle',\n",
       "  u'giant',\n",
       "  u'(',\n",
       "  u'ving',\n",
       "  u'rhames',\n",
       "  u')',\n",
       "  u'.',\n",
       "  u'\\nthrowing',\n",
       "  u'a',\n",
       "  u'monkey',\n",
       "  u'wrench',\n",
       "  u'into',\n",
       "  u'the',\n",
       "  u'plans',\n",
       "  u'is',\n",
       "  u'the',\n",
       "  u'requisite',\n",
       "  u'dumb',\n",
       "  u'ass',\n",
       "  u':',\n",
       "  u'in',\n",
       "  u'this',\n",
       "  u'case',\n",
       "  u'general',\n",
       "  u'hein',\n",
       "  u'(',\n",
       "  u'james',\n",
       "  u'woods',\n",
       "  u')',\n",
       "  u',',\n",
       "  u'who',\n",
       "  u'wants',\n",
       "  u'to',\n",
       "  u'use',\n",
       "  u'the',\n",
       "  u'zeus',\n",
       "  u'cannon',\n",
       "  u'to',\n",
       "  u'bomb',\n",
       "  u'the',\n",
       "  u'aliens',\n",
       "  u'back',\n",
       "  u'to',\n",
       "  u'the',\n",
       "  u'stone',\n",
       "  u'age',\n",
       "  u',',\n",
       "  u'even',\n",
       "  u'if',\n",
       "  u'it',\n",
       "  u'destroys',\n",
       "  u'earth',\n",
       "  u'as',\n",
       "  u'well',\n",
       "  u'.',\n",
       "  u'\\nso',\n",
       "  u'there',\n",
       "  u'you',\n",
       "  u'have',\n",
       "  u'it',\n",
       "  u'.',\n",
       "  u'\\nlike',\n",
       "  u'most',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'anime',\n",
       "  u\"i've\",\n",
       "  u'seen',\n",
       "  u',',\n",
       "  u'the',\n",
       "  u'plot',\n",
       "  u'combines',\n",
       "  u'apocalyptic',\n",
       "  u'settings',\n",
       "  u',',\n",
       "  u'lots',\n",
       "  u'of',\n",
       "  u'shooting',\n",
       "  u'and',\n",
       "  u'fuzzy',\n",
       "  u'spirituality',\n",
       "  u',',\n",
       "  u'all',\n",
       "  u'wrapped',\n",
       "  u'up',\n",
       "  u'in',\n",
       "  u'a',\n",
       "  u'save-the-earth',\n",
       "  u'bow',\n",
       "  u'.',\n",
       "  u'\\nbut',\n",
       "  u\"i'm\",\n",
       "  u'bored',\n",
       "  u'with',\n",
       "  u'apocalyptic',\n",
       "  u'settings',\n",
       "  u'.',\n",
       "  u'\\ni',\n",
       "  u'understand',\n",
       "  u'why',\n",
       "  u'so',\n",
       "  u'many',\n",
       "  u'live-action',\n",
       "  u'films',\n",
       "  u'employ',\n",
       "  u'them',\n",
       "  u'?',\n",
       "  u\"they're\",\n",
       "  u'cheap',\n",
       "  u'?',\n",
       "  u'but',\n",
       "  u'animated',\n",
       "  u'films',\n",
       "  u'can',\n",
       "  u'show',\n",
       "  u'anything',\n",
       "  u',',\n",
       "  u'so',\n",
       "  u'why',\n",
       "  u'wallow',\n",
       "  u'in',\n",
       "  u'an',\n",
       "  u'industrial',\n",
       "  u'trash',\n",
       "  u'heap',\n",
       "  u'?',\n",
       "  u'\\nthe',\n",
       "  u'action',\n",
       "  u'scenes',\n",
       "  u'and',\n",
       "  u'shoot-em-ups',\n",
       "  u\"don't\",\n",
       "  u'satisfy',\n",
       "  u'either',\n",
       "  u'.',\n",
       "  u'\\nthe',\n",
       "  u'humans',\n",
       "  u'move',\n",
       "  u'oddly',\n",
       "  u'and',\n",
       "  u'their',\n",
       "  u'facial',\n",
       "  u'features',\n",
       "  u'are',\n",
       "  u'so',\n",
       "  u'muted',\n",
       "  u'that',\n",
       "  u'the',\n",
       "  u'talented',\n",
       "  u'voice',\n",
       "  u'cast',\n",
       "  u\"can't\",\n",
       "  u'bring',\n",
       "  u'them',\n",
       "  u'to',\n",
       "  u'life',\n",
       "  u'(',\n",
       "  u'in',\n",
       "  u'fact',\n",
       "  u',',\n",
       "  u'their',\n",
       "  u'efforts',\n",
       "  u'merely',\n",
       "  u'emphasize',\n",
       "  u'what',\n",
       "  u\"we're\",\n",
       "  u'missing',\n",
       "  u')',\n",
       "  u'.',\n",
       "  u'\\naki',\n",
       "  u'is',\n",
       "  u'especially',\n",
       "  u'disappointing',\n",
       "  u';',\n",
       "  u'with',\n",
       "  u'her',\n",
       "  u'lack',\n",
       "  u'of',\n",
       "  u'expression',\n",
       "  u'and',\n",
       "  u'flat',\n",
       "  u'delivery',\n",
       "  u',',\n",
       "  u'she',\n",
       "  u'looks',\n",
       "  u'and',\n",
       "  u'sound',\n",
       "  u'like',\n",
       "  u'a',\n",
       "  u'brunet',\n",
       "  u'version',\n",
       "  u'of',\n",
       "  u'weena',\n",
       "  u',',\n",
       "  u'the',\n",
       "  u'eloi',\n",
       "  u'girl',\n",
       "  u'from',\n",
       "  u\"1960's\",\n",
       "  u'\"',\n",
       "  u'the',\n",
       "  u'time',\n",
       "  u'machine',\n",
       "  u'.',\n",
       "  u'\"',\n",
       "  u'\\ndrab',\n",
       "  u'color',\n",
       "  u'choices',\n",
       "  u'and',\n",
       "  u'aliens',\n",
       "  u'that',\n",
       "  u'appear',\n",
       "  u'to',\n",
       "  u'have',\n",
       "  u'been',\n",
       "  u'created',\n",
       "  u'in',\n",
       "  u'jell-o',\n",
       "  u'molds',\n",
       "  u'sap',\n",
       "  u'the',\n",
       "  u'pizzazz',\n",
       "  u'from',\n",
       "  u'the',\n",
       "  u'big',\n",
       "  u'set',\n",
       "  u'pieces',\n",
       "  u'.',\n",
       "  u'\\nstudents',\n",
       "  u'of',\n",
       "  u'computer',\n",
       "  u'animation',\n",
       "  u'may',\n",
       "  u'be',\n",
       "  u'fascinated',\n",
       "  u'with',\n",
       "  u'the',\n",
       "  u'technology',\n",
       "  u'behind',\n",
       "  u'\"',\n",
       "  u'final',\n",
       "  u'fantasy',\n",
       "  u':',\n",
       "  u'the',\n",
       "  u'spirits',\n",
       "  u'within',\n",
       "  u',',\n",
       "  u'\"',\n",
       "  u'but',\n",
       "  u'i',\n",
       "  u'found',\n",
       "  u'it',\n",
       "  u'sub-par',\n",
       "  u'across',\n",
       "  u'the',\n",
       "  u'board',\n",
       "  u'.',\n",
       "  u'\\n',\n",
       "  u'\"',\n",
       "  u'futurama',\n",
       "  u'\"',\n",
       "  u'does',\n",
       "  u'more',\n",
       "  u'effective',\n",
       "  u'battle',\n",
       "  u'visuals',\n",
       "  u',',\n",
       "  u'the',\n",
       "  u'kids',\n",
       "  u'in',\n",
       "  u'\"',\n",
       "  u'south',\n",
       "  u'park',\n",
       "  u'\"',\n",
       "  u'are',\n",
       "  u'far',\n",
       "  u'more',\n",
       "  u'expressive',\n",
       "  u'than',\n",
       "  u'these',\n",
       "  u'mannequins',\n",
       "  u'and',\n",
       "  u'any',\n",
       "  u'old',\n",
       "  u'episode',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  ...]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(1048576, {19: 1.0, 2424: 1.0, 3015: 1.0, 3932: 12.0, 4415: 7.0, 6292: 1.0, 9314: 1.0, 13954: 1.0, 15590: 1.0, 18549: 1.0, 24165: 5.0, 24208: 1.0, 28000: 1.0, 28017: 2.0, 36751: 2.0, 36754: 2.0, 36757: 2.0, 38019: 1.0, 38034: 1.0, 43230: 1.0, 48642: 1.0, 50570: 14.0, 50573: 6.0, 50583: 12.0, 50591: 1.0, 52134: 1.0, 52237: 1.0, 52431: 1.0, 57166: 5.0, 57787: 1.0, 64361: 1.0, 66096: 1.0, 68798: 1.0, 69414: 4.0, 69913: 1.0, 74605: 2.0, 75990: 1.0, 76149: 2.0, 80932: 2.0, 81775: 3.0, 84450: 1.0, 85077: 1.0, 90808: 1.0, 91755: 1.0, 93759: 1.0, 94976: 1.0, 97255: 1.0, 104251: 4.0, 104426: 1.0, 110080: 1.0, 110522: 4.0, 113471: 1.0, 115245: 46.0, 117402: 1.0, 117409: 1.0, 123489: 1.0, 124562: 1.0, 124666: 1.0, 126987: 1.0, 129375: 1.0, 132052: 1.0, 134398: 1.0, 136113: 1.0, 141912: 4.0, 146975: 1.0, 149770: 1.0, 156287: 1.0, 158643: 1.0, 159034: 1.0, 160065: 1.0, 160915: 1.0, 167617: 1.0, 168184: 2.0, 171836: 1.0, 178362: 2.0, 180063: 1.0, 183576: 1.0, 190145: 1.0, 191617: 1.0, 192581: 1.0, 194865: 1.0, 196531: 1.0, 197764: 1.0, 199528: 1.0, 202225: 1.0, 210493: 1.0, 213409: 1.0, 213913: 2.0, 215042: 1.0, 215842: 1.0, 216693: 2.0, 216768: 2.0, 216815: 1.0, 217348: 2.0, 222396: 1.0, 225801: 1.0, 232347: 1.0, 233186: 1.0, 234013: 1.0, 237522: 1.0, 240455: 8.0, 240477: 4.0, 243340: 1.0, 244883: 2.0, 245892: 3.0, 246320: 1.0, 248010: 1.0, 250914: 1.0, 251465: 1.0, 251525: 1.0, 263336: 1.0, 263471: 47.0, 263739: 2.0, 263760: 1.0, 270084: 1.0, 272408: 1.0, 283887: 1.0, 285196: 1.0, 286954: 1.0, 292800: 2.0, 292818: 1.0, 293156: 4.0, 293604: 2.0, 295940: 1.0, 296362: 1.0, 297432: 1.0, 298274: 1.0, 298293: 1.0, 299392: 1.0, 316756: 1.0, 318420: 1.0, 319572: 1.0, 320552: 1.0, 322824: 1.0, 322951: 1.0, 323100: 1.0, 323297: 7.0, 323305: 31.0, 323651: 2.0, 324571: 1.0, 327909: 1.0, 328158: 1.0, 332003: 7.0, 332247: 2.0, 332965: 4.0, 337090: 1.0, 338085: 3.0, 338895: 1.0, 343190: 1.0, 344097: 1.0, 345235: 1.0, 345551: 1.0, 346622: 1.0, 346946: 1.0, 347143: 2.0, 348047: 2.0, 348461: 1.0, 351053: 1.0, 352887: 1.0, 360795: 1.0, 361863: 1.0, 361897: 1.0, 362808: 2.0, 363623: 1.0, 364357: 1.0, 366119: 1.0, 368307: 2.0, 369003: 1.0, 370607: 1.0, 371882: 1.0, 372509: 1.0, 376269: 1.0, 379763: 1.0, 379800: 1.0, 381981: 1.0, 385180: 2.0, 387045: 1.0, 389192: 1.0, 389204: 1.0, 392560: 3.0, 396062: 1.0, 407772: 1.0, 408353: 1.0, 409241: 1.0, 411082: 1.0, 412025: 1.0, 412196: 2.0, 415203: 1.0, 415281: 1.0, 416351: 1.0, 418086: 1.0, 418577: 5.0, 419951: 1.0, 421213: 3.0, 421708: 1.0, 422691: 24.0, 425558: 2.0, 426980: 1.0, 429834: 1.0, 431272: 1.0, 433944: 1.0, 434754: 3.0, 434774: 2.0, 435068: 1.0, 436127: 1.0, 441177: 1.0, 441832: 3.0, 444318: 1.0, 444709: 1.0, 445492: 1.0, 445618: 1.0, 446841: 1.0, 449317: 1.0, 453276: 1.0, 456720: 1.0, 457040: 1.0, 464941: 1.0, 468109: 1.0, 469427: 1.0, 470089: 1.0, 470575: 1.0, 472376: 2.0, 474252: 2.0, 474814: 10.0, 476766: 1.0, 479134: 1.0, 483648: 1.0, 488917: 1.0, 491621: 1.0, 492547: 1.0, 493161: 1.0, 494287: 1.0, 496069: 1.0, 498063: 1.0, 500996: 1.0, 501708: 1.0, 507771: 1.0, 515995: 1.0, 516867: 1.0, 518729: 1.0, 521364: 1.0, 521365: 7.0, 522303: 1.0, 522761: 3.0, 524103: 2.0, 524515: 1.0, 525105: 1.0, 531940: 1.0, 535067: 1.0, 539841: 1.0, 539862: 1.0, 542959: 1.0, 543082: 1.0, 543933: 1.0, 546055: 1.0, 550765: 1.0, 550849: 1.0, 551948: 1.0, 555979: 3.0, 555997: 2.0, 557488: 1.0, 558402: 2.0, 559480: 1.0, 565072: 1.0, 567029: 2.0, 568904: 1.0, 570048: 1.0, 571636: 1.0, 572533: 1.0, 573643: 1.0, 578189: 1.0, 581984: 4.0, 584310: 1.0, 587573: 1.0, 595821: 1.0, 596881: 1.0, 598759: 1.0, 603493: 1.0, 605676: 1.0, 606981: 2.0, 608712: 1.0, 608735: 1.0, 610190: 1.0, 610596: 2.0, 611148: 1.0, 613712: 1.0, 614102: 1.0, 614645: 1.0, 615501: 1.0, 616461: 2.0, 617569: 1.0, 618498: 4.0, 619310: 1.0, 620569: 1.0, 620971: 1.0, 620983: 4.0, 622011: 1.0, 622133: 1.0, 623388: 1.0, 625767: 2.0, 625876: 1.0, 628962: 1.0, 629096: 2.0, 637757: 5.0, 639136: 2.0, 644238: 1.0, 645741: 1.0, 645886: 1.0, 648662: 1.0, 651783: 1.0, 654703: 1.0, 658752: 1.0, 659803: 1.0, 665247: 1.0, 668871: 1.0, 669304: 1.0, 669775: 3.0, 672251: 1.0, 672287: 1.0, 674865: 1.0, 674994: 1.0, 679058: 1.0, 680954: 1.0, 681165: 1.0, 686096: 3.0, 687117: 1.0, 688377: 1.0, 688378: 1.0, 688614: 1.0, 690866: 1.0, 692065: 1.0, 692725: 1.0, 693028: 1.0, 697409: 1.0, 697436: 3.0, 697723: 2.0, 698560: 1.0, 699621: 1.0, 706507: 1.0, 706629: 1.0, 708231: 2.0, 710452: 1.0, 710459: 2.0, 712548: 1.0, 714030: 1.0, 715648: 28.0, 715677: 3.0, 719381: 1.0, 720366: 1.0, 725032: 1.0, 725041: 16.0, 728047: 1.0, 728062: 1.0, 730632: 1.0, 731696: 1.0, 733639: 1.0, 734802: 1.0, 735780: 1.0, 736353: 1.0, 741131: 3.0, 742235: 1.0, 744786: 1.0, 745424: 1.0, 748130: 1.0, 749730: 1.0, 750143: 1.0, 750992: 1.0, 753547: 1.0, 758603: 1.0, 759723: 1.0, 761363: 1.0, 761452: 1.0, 767355: 2.0, 770189: 1.0, 772394: 1.0, 772686: 1.0, 773863: 2.0, 778603: 2.0, 779289: 1.0, 780314: 1.0, 780822: 1.0, 781531: 1.0, 781532: 1.0, 783361: 1.0, 786838: 1.0, 787763: 1.0, 789916: 1.0, 792330: 1.0, 796794: 1.0, 799175: 1.0, 801592: 1.0, 802840: 1.0, 803019: 1.0, 803829: 1.0, 804209: 1.0, 805412: 1.0, 805631: 1.0, 807330: 1.0, 808340: 1.0, 809878: 6.0, 810221: 1.0, 815906: 1.0, 821366: 1.0, 822524: 1.0, 823168: 2.0, 824370: 1.0, 824695: 1.0, 825062: 2.0, 826136: 1.0, 826827: 1.0, 828413: 1.0, 831560: 1.0, 832566: 1.0, 838142: 1.0, 845654: 1.0, 846015: 2.0, 852523: 1.0, 852617: 1.0, 854516: 3.0, 855273: 1.0, 856697: 1.0, 858635: 2.0, 859153: 1.0, 861749: 1.0, 867369: 10.0, 869261: 1.0, 869910: 3.0, 873821: 1.0, 883976: 1.0, 885909: 2.0, 886775: 2.0, 889021: 1.0, 889976: 1.0, 891016: 1.0, 897504: 28.0, 899604: 1.0, 901933: 1.0, 906412: 1.0, 907018: 4.0, 907365: 1.0, 913007: 1.0, 913339: 1.0, 918896: 1.0, 923251: 1.0, 923375: 1.0, 925985: 2.0, 931270: 1.0, 934746: 1.0, 935223: 1.0, 937312: 1.0, 938455: 1.0, 938516: 1.0, 939888: 1.0, 941480: 10.0, 944044: 1.0, 945963: 1.0, 946355: 2.0, 947825: 1.0, 948889: 1.0, 948895: 4.0, 949115: 1.0, 949848: 1.0, 951974: 2.0, 952496: 1.0, 954722: 1.0, 954775: 1.0, 956097: 4.0, 956125: 2.0, 958706: 3.0, 959994: 53.0, 960418: 1.0, 963733: 1.0, 968518: 1.0, 972213: 1.0, 972429: 2.0, 975711: 1.0, 979736: 1.0, 981282: 1.0, 988522: 2.0, 993084: 1.0, 993085: 1.0, 994354: 1.0, 994664: 1.0, 996759: 1.0, 997131: 2.0, 1000947: 1.0, 1002631: 1.0, 1004497: 1.0, 1008437: 1.0, 1009065: 1.0, 1010466: 1.0, 1014577: 5.0, 1014710: 1.0, 1017476: 1.0, 1017759: 1.0, 1021097: 3.0, 1026665: 1.0, 1032951: 1.0, 1034253: 1.0, 1035237: 1.0, 1036867: 1.0, 1042402: 1.0, 1043258: 1.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import IDF\n",
    "\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#labeledTfidfPoints.map(lambda a:a.features).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#make labeledpoints\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "#?LabeledPoint\n",
    "labeledTfidfPoints=tfidf.zipWithIndex().map(lambda (a,b):(b,a)).join(data.map(lambda a:a[0]).zipWithIndex().map(lambda (a,b):(b,a))).map(lambda (a,b):LabeledPoint(b[1],b[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#labeledpoint function\n",
    "def giveLabeledPoints(keyValuePair):\n",
    "    return LabeledPoint(keyValuePair[0],keyValuePair[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create labeledpoints by using map on the rdd\n",
    "#labeledTfidfPoints=labeledTfidf.map(lambda a:giveLabeledPoints(a))\n",
    "#labeledTfidfPoints=labeledTfidf.map(lambda a:LabeledPoint(a[0],a[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create training, validation and testing data\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "labeledTfidfPoints.cache()\n",
    "rawTrainData, rawValidationData, rawTestData = labeledTfidfPoints.randomSplit(weights,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[28] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawTrainData.cache()\n",
    "rawValidationData.cache()\n",
    "rawTestData.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "\n",
    "svm = SVMWithSGD.train(data=rawTrainData, iterations=10, step=1.0, regParam=0.01, miniBatchFraction=1.0, initialWeights=None, regType='l2', intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9956140350877193"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmLabelsAndPredsTrain = rawTrainData.map(lambda p: (p.label, svm.predict(p.features)))\n",
    "#Overall True Prediction Rate\n",
    "svmLabelsAndPredsTrain.filter(lambda (a,b):a==b).count()/float(rawTrainData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996216897856\n",
      "0.99501867995\n"
     ]
    }
   ],
   "source": [
    "#True Negatives\n",
    "print svmLabelsAndPredsTrain.filter(lambda (a,b):a==0).filter(lambda (a,b):a==b).count()/float(rawTrainData.filter(lambda a:a.label==0).count())\n",
    "#True Positives\n",
    "print svmLabelsAndPredsTrain.filter(lambda (a,b):a==1).filter(lambda (a,b):a==b).count()/float(rawTrainData.filter(lambda a:a.label==1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=sc.parallelize(tfidf.zipWithIndex().map(lambda (a,b):(b,a)).take(3))\n",
    "b=sc.parallelize(data.map(lambda a:a[0]).zipWithIndex().map(lambda (a,b):(b,a)).take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  SparseVector(1048576, {19: 3.7513, 2424: 2.6316, 3015: 3.2194, 3932: 0.3839, 4415: 2.1873, 6292: 2.1898, 9314: 2.4029, 13954: 4.5569, 15590: 4.5569, 18549: 6.2151, 24165: 1.6415, 24208: 3.0581, 28000: 1.8648, 28017: 2.6123, 36751: 0.2442, 36754: 0.0973, 36757: 0.3426, 38019: 1.533, 38034: 0.7428, 43230: 2.4253, 48642: 6.5028, 50570: 0.035, 50573: 0.2761, 50583: 0.0421, 50591: 0.5478, 52134: 5.6555, 52237: 3.8172, 52431: 2.8919, 57166: 0.4741, 57787: 4.4233, 64361: 2.2308, 66096: 4.1049, 68798: 5.992, 69414: 2.4006, 69913: 3.5071, 74605: 1.9335, 75990: 5.2035, 76149: 1.021, 80932: 3.962, 81775: 20.7248, 84450: 4.8934, 85077: 2.0759, 90808: 6.9083, 91755: 3.7728, 93759: 4.5104, 94976: 5.1165, 97255: 6.5028, 104251: 1.9013, 104426: 2.571, 110080: 5.992, 110522: 1.5535, 113471: 5.0365, 115245: 0.069, 117402: 1.9489, 117409: 2.3544, 123489: 5.2988, 124562: 4.2692, 124666: 3.4905, 126987: 2.2687, 129375: 2.2833, 132052: 3.1706, 134398: 1.4009, 136113: 0.6987, 141912: 0.7861, 146975: 4.3825, 149770: 2.6741, 156287: 4.3825, 158643: 3.3529, 159034: 3.3109, 160065: 2.8392, 160915: 3.8878, 167617: 4.4659, 168184: 2.6458, 171836: 6.5028, 178362: 1.9414, 180063: 0.2535, 183576: 5.6555, 190145: 3.5071, 191617: 3.6894, 192581: 2.3131, 194865: 1.0575, 196531: 0.6241, 197764: 6.9083, 199528: 4.1357, 202225: 1.9956, 210493: 3.6696, 213409: 4.0179, 213913: 2.3209, 215042: 2.5388, 215842: 0.8057, 216693: 3.7622, 216768: 10.5976, 216815: 5.0365, 217348: 5.3339, 222396: 2.8919, 225801: 3.1706, 232347: 5.522, 233186: 3.2707, 234013: 0.6354, 237522: 2.3597, 240455: 0.1738, 240477: 1.8154, 243340: 0.4168, 244883: 5.6956, 245892: 8.2248, 246320: 1.1001, 248010: 6.2151, 250914: 2.8829, 251465: 1.1945, 251525: 1.9595, 263336: 2.8565, 263471: 0.0, 263739: 0.4398, 263760: 3.4743, 270084: 4.5104, 272408: 2.571, 283887: 3.6502, 285196: 6.2151, 286954: 2.4656, 292800: 2.583, 292818: 0.6867, 293156: 0.6757, 293604: 11.6193, 295940: 3.6894, 296362: 6.9083, 297432: 3.0164, 298274: 4.5569, 298293: 3.8402, 299392: 2.2078, 316756: 2.5975, 318420: 3.6894, 319572: 1.1701, 320552: 4.9623, 322824: 3.3247, 322951: 5.2035, 323100: 3.8637, 323297: 0.5644, 323305: 0.031, 323651: 3.4586, 324571: 2.0523, 327909: 6.9083, 328158: 3.8878, 332003: 0.4217, 332247: 8.3348, 332965: 20.1458, 337090: 0.7684, 338085: 7.8534, 338895: 6.9083, 343190: 3.1241, 344097: 1.7042, 345235: 3.0164, 345551: 4.9623, 346622: 2.4196, 346946: 2.1809, 347143: 2.9754, 348047: 5.2912, 348461: 0.6877, 351053: 4.3825, 352887: 1.4944, 360795: 2.7974, 361863: 1.3043, 361897: 1.2681, 362808: 1.7793, 363623: 3.3673, 364357: 2.4253, 366119: 6.2151, 368307: 1.4878, 369003: 1.0319, 370607: 2.7651, 371882: 4.4659, 372509: 1.2897, 376269: 2.5975, 379763: 5.6555, 379800: 4.1049, 381981: 1.2436, 385180: 9.5364, 387045: 2.8565, 389192: 1.4572, 389204: 2.7651, 392560: 1.8977, 396062: 1.8425, 407772: 6.9083, 408353: 2.092, 409241: 1.62, 411082: 0.937, 412025: 1.4789, 412196: 4.0358, 415203: 3.0475, 415281: 3.5584, 416351: 6.9083, 418086: 0.5893, 418577: 16.5547, 419951: 4.0461, 421213: 9.8517, 421708: 4.0179, 422691: 6.2692, 425558: 8.4682, 426980: 3.1826, 429834: 3.0905, 431272: 5.2988, 433944: 6.9083, 434754: 1.9719, 434774: 0.7518, 435068: 4.0461, 436127: 3.1588, 441177: 5.0365, 441832: 1.0269, 444318: 4.4659, 444709: 4.4659, 445492: 1.9738, 445618: 1.891, 446841: 5.992, 449317: 4.4233, 453276: 6.9083, 456720: 2.9286, 457040: 6.5028, 464941: 1.3868, 468109: 1.9595, 469427: 0.7179, 470089: 5.2988, 470575: 6.9083, 472376: 10.0729, 474252: 6.2255, 474814: 4.8745, 476766: 3.0371, 479134: 0.149, 483648: 4.4233, 488917: 6.9083, 491621: 5.2988, 492547: 6.5028, 493161: 6.9083, 494287: 1.4594, 496069: 3.1588, 498063: 3.1471, 500996: 3.1016, 501708: 2.7572, 507771: 2.2735, 515995: 3.207, 516867: 4.4233, 518729: 5.522, 521364: 2.8919, 521365: 0.8192, 522303: 6.5028, 522761: 4.3652, 524103: 4.2167, 524515: 2.2882, 525105: 3.0063, 531940: 0.8057, 535067: 1.1291, 539841: 4.657, 539862: 2.938, 542959: 5.2988, 543082: 5.4042, 543933: 2.3284, 546055: 1.4029, 550765: 5.992, 550849: 1.643, 551948: 2.8478, 555979: 0.4557, 555997: 2.7537, 557488: 3.1947, 558402: 1.8997, 559480: 3.9125, 565072: 3.9638, 567029: 11.311, 568904: 2.5515, 570048: 3.3673, 571636: 4.657, 572533: 0.88, 573643: 0.874, 578189: 1.8844, 581984: 3.8934, 584310: 2.1125, 587573: 2.5325, 595821: 0.3486, 596881: 4.3433, 598759: 2.5841, 603493: 1.2281, 605676: 0.7946, 606981: 4.8732, 608712: 1.2332, 608735: 0.9681, 610190: 2.4597, 610596: 5.6785, 611148: 3.0475, 613712: 4.4659, 614102: 1.7463, 614645: 4.657, 615501: 5.2988, 616461: 5.3923, 617569: 3.3819, 618498: 0.9958, 619310: 4.8934, 620569: 4.0461, 620971: 2.9474, 620983: 20.1458, 622011: 1.126, 622133: 3.3673, 623388: 4.5569, 625767: 8.9318, 625876: 4.2341, 628962: 1.0676, 629096: 0.1645, 637757: 9.5215, 639136: 1.0478, 644238: 4.9623, 645741: 5.6555, 645886: 3.5239, 648662: 0.7428, 651783: 1.0029, 654703: 3.7096, 658752: 1.6692, 659803: 4.2002, 665247: 3.207, 668871: 2.957, 669304: 2.5262, 669775: 12.0537, 672251: 5.2035, 672287: 1.0518, 674865: 5.2988, 674994: 1.1061, 679058: 3.541, 680954: 2.0331, 681165: 1.6852, 686096: 8.2481, 687117: 5.992, 688377: 3.4583, 688378: 1.8844, 688614: 2.4834, 690866: 4.2692, 692065: 2.0446, 692725: 5.6555, 693028: 3.1355, 697409: 0.6401, 697436: 8.1332, 697723: 5.0153, 698560: 4.2341, 699621: 2.7111, 706507: 2.5452, 706629: 3.1826, 708231: 9.3139, 710452: 5.2988, 710459: 6.5413, 712548: 3.5584, 714030: 0.6102, 715648: 0.028, 715677: 2.2507, 719381: 1.6906, 720366: 3.1241, 725032: 2.1987, 725041: 0.024, 728047: 2.0369, 728062: 1.2418, 730632: 5.2035, 731696: 4.0179, 733639: 6.5028, 734802: 2.3491, 735780: 4.3056, 736353: 4.8934, 741131: 0.0, 742235: 1.9454, 744786: 4.0461, 745424: 5.8096, 748130: 3.9378, 749730: 0.4761, 750143: 1.3361, 750992: 3.2194, 753547: 6.9083, 758603: 6.2151, 759723: 5.2035, 761363: 6.9083, 761452: 1.7435, 767355: 6.3412, 770189: 3.7728, 772394: 6.9083, 772686: 4.5104, 773863: 13.0056, 778603: 3.4586, 779289: 2.52, 780314: 4.1674, 780822: 4.2341, 781531: 4.5104, 781532: 1.2315, 783361: 6.2151, 786838: 1.5011, 787763: 0.5992, 789916: 3.5071, 792330: 0.4259, 796794: 1.62, 799175: 5.992, 801592: 3.9905, 802840: 3.8402, 803019: 1.8778, 803829: 0.7577, 804209: 3.4583, 805412: 2.4029, 805631: 0.7169, 807330: 2.3131, 808340: 6.5028, 809878: 1.2341, 810221: 3.7947, 815906: 0.6373, 821366: 4.3825, 822524: 3.8637, 823168: 7.9276, 824370: 5.2988, 824695: 1.7903, 825062: 2.7977, 826136: 3.2839, 826827: 4.7682, 828413: 3.8172, 831560: 0.7324, 832566: 1.6378, 838142: 5.1165, 845654: 4.9623, 846015: 8.6111, 852523: 1.83, 852617: 1.852, 854516: 1.0973, 855273: 0.972, 856697: 1.8084, 858635: 5.116, 859153: 2.2882, 861749: 3.3967, 867369: 0.5127, 869261: 3.8637, 869910: 5.8362, 873821: 6.2151, 883976: 3.5941, 885909: 4.5962, 886775: 0.9863, 889021: 4.2002, 889976: 5.522, 891016: 3.4583, 897504: 0.056, 899604: 2.1547, 901933: 3.6894, 906412: 3.3529, 907018: 12.1482, 907365: 4.3433, 913007: 3.9125, 913339: 3.7096, 918896: 4.7682, 923251: 3.9378, 923375: 6.2151, 925985: 1.682, 931270: 4.9623, 934746: 4.657, 935223: 2.9667, 937312: 2.381, 938455: 2.1853, 938516: 0.913, 939888: 2.7651, 941480: 0.5074, 944044: 0.4421, 945963: 0.626, 946355: 4.5277, 947825: 4.657, 948889: 1.62, 948895: 1.1368, 949115: 6.9083, 949848: 3.4743, 951974: 0.0879, 952496: 3.4583, 954722: 3.2707, 954775: 1.5926, 956097: 0.4884, 956125: 0.2533, 958706: 18.6453, 959994: 0.0265, 960418: 2.6178, 963733: 5.4042, 968518: 2.4196, 972213: 5.6555, 972429: 2.0807, 975711: 6.9083, 979736: 3.232, 981282: 1.8084, 988522: 5.0903, 993084: 0.2935, 993085: 2.4539, 994354: 0.4531, 994664: 0.9894, 996759: 5.0365, 997131: 4.8847, 1000947: 5.2988, 1002631: 6.9083, 1004497: 0.5679, 1008437: 0.7781, 1009065: 3.541, 1010466: 1.6378, 1014577: 2.2692, 1014710: 0.4267, 1017476: 1.3572, 1017759: 6.2151, 1021097: 0.815, 1026665: 1.9666, 1032951: 5.2035, 1034253: 4.3825, 1035237: 1.7098, 1036867: 0.5591, 1042402: 3.1355, 1043258: 2.3335})),\n",
       " (1,\n",
       "  SparseVector(1048576, {25: 3.4425, 3932: 0.096, 6890: 0.8375, 16741: 4.5568, 17899: 4.3825, 20838: 2.1677, 24165: 1.3132, 28707: 3.1826, 35388: 3.8402, 36751: 0.7326, 36754: 0.1459, 49806: 2.4774, 50570: 0.005, 50573: 0.092, 50583: 0.028, 50591: 0.5478, 50843: 6.9083, 52654: 1.1573, 57166: 0.2844, 62214: 2.8652, 64361: 4.4615, 69189: 1.0837, 69414: 4.2011, 76149: 1.021, 91349: 3.7302, 93454: 3.1128, 95115: 3.9378, 110522: 1.1651, 111151: 1.2915, 115245: 0.0255, 117409: 2.3544, 118063: 2.6887, 127777: 3.8637, 127798: 3.7728, 136113: 0.6987, 147451: 5.0365, 151782: 6.9083, 153651: 6.9083, 158820: 6.5028, 167933: 1.5611, 170196: 2.1809, 173518: 4.1124, 191439: 2.2354, 191699: 4.9623, 197912: 5.2035, 199577: 3.4425, 200206: 1.696, 202225: 1.9956, 209189: 1.321, 212569: 2.6741, 213865: 4.8934, 213913: 1.1605, 217599: 1.5192, 220072: 20.466, 220552: 1.2593, 220898: 1.2691, 227417: 0.7935, 232338: 1.2247, 234013: 1.2708, 240455: 0.0217, 243340: 0.8335, 251465: 1.1945, 251482: 1.5635, 258609: 4.5569, 263471: 0.0, 263739: 0.2199, 292818: 1.3733, 293156: 0.1689, 295675: 6.5028, 307883: 1.3888, 307928: 1.7181, 311687: 1.5124, 316816: 5.8096, 320173: 4.8288, 323297: 0.1613, 323305: 0.01, 330870: 6.9083, 332003: 0.1205, 332302: 6.9083, 332578: 2.365, 335307: 4.2692, 348708: 3.8637, 354917: 1.852, 355606: 5.992, 358352: 4.0179, 362808: 0.8897, 362994: 5.8096, 372000: 2.7416, 372509: 1.2897, 378061: 6.9083, 381421: 4.4659, 381507: 1.901, 383731: 6.9083, 387072: 9.0207, 388651: 4.1674, 394210: 3.4743, 405578: 2.2261, 414428: 2.1721, 419033: 3.0905, 422691: 1.5673, 426015: 4.3825, 430441: 2.8652, 437374: 2.5775, 437502: 1.2384, 441832: 0.3423, 449055: 2.6247, 458315: 4.4247, 458945: 6.5028, 471144: 3.4984, 471859: 2.4423, 474814: 0.4874, 477796: 2.3919, 479134: 0.298, 506630: 5.2035, 512935: 3.8172, 523861: 0.984, 537764: 3.6894, 545376: 2.6669, 546555: 5.992, 553919: 1.9144, 554798: 2.4596, 555979: 0.3038, 558402: 0.9498, 558727: 0.8716, 568804: 2.4084, 570503: 4.9623, 572533: 5.2799, 573757: 1.963, 581984: 0.9734, 595821: 0.6973, 595965: 1.6, 602709: 2.6247, 605676: 0.7946, 608735: 0.9681, 612939: 4.4659, 613313: 5.522, 618498: 0.4979, 623736: 2.5077, 624182: 2.4029, 629096: 0.2468, 640114: 1.9414, 654419: 2.571, 659600: 3.9404, 664379: 3.0112, 684678: 3.7513, 690094: 1.62, 693005: 2.414, 697409: 0.6401, 697909: 6.9083, 699198: 1.901, 706277: 4.5569, 706507: 2.5452, 714030: 1.2205, 715648: 0.012, 719381: 1.6906, 722648: 3.6124, 725041: 0.009, 732982: 1.3828, 733692: 2.874, 741131: 0.0, 749100: 3.216, 750143: 1.3361, 753981: 1.7153, 773886: 2.9764, 774759: 3.8878, 777701: 5.6555, 778001: 5.522, 780469: 6.5028, 781528: 5.0365, 782310: 3.0475, 783613: 3.919, 792330: 0.8518, 793412: 4.4233, 803680: 1.83, 803829: 0.7577, 805631: 0.7169, 807892: 2.9101, 809878: 0.617, 810281: 0.6158, 810598: 2.365, 822131: 2.4029, 834094: 5.2035, 845243: 1.852, 846159: 4.7682, 853558: 3.1016, 854516: 0.3658, 855317: 4.7682, 856375: 4.0461, 858635: 2.558, 864122: 6.9083, 865981: 4.5569, 867369: 0.2051, 873178: 6.7058, 878608: 2.2123, 886775: 0.9863, 887932: 0.799, 896710: 2.5644, 897504: 0.028, 897559: 4.0179, 898918: 5.1165, 900470: 5.4042, 901185: 5.4042, 902523: 3.6894, 914273: 3.6311, 917158: 2.1853, 923992: 9.2064, 929590: 1.8207, 935566: 4.1357, 941480: 0.203, 944044: 0.8842, 948895: 0.2842, 951974: 0.2196, 959994: 0.011, 965192: 5.992, 965468: 5.3339, 967859: 1.5146, 969973: 4.5569, 971063: 13.8165, 971574: 5.0365, 985128: 6.9083, 993078: 1.3714, 993084: 0.5871, 994354: 0.9061, 1001957: 4.2692, 1004334: 1.3266, 1008437: 2.3343, 1014710: 0.4267, 1017476: 1.3572, 1021097: 0.815, 1033734: 0.4351, 1036867: 0.5591})),\n",
       " (2,\n",
       "  SparseVector(1048576, {3932: 0.032, 4415: 0.3125, 6890: 1.675, 13537: 8.1501, 17857: 2.381, 19527: 0.6204, 22133: 4.6057, 24165: 0.3283, 27014: 2.4894, 31003: 6.2151, 31829: 6.5028, 33950: 0.8788, 36754: 0.3405, 36757: 0.5139, 50570: 0.025, 50583: 0.0175, 57166: 0.2844, 66793: 2.8223, 69189: 1.0837, 69414: 1.2003, 70753: 3.8637, 72232: 3.3109, 73137: 3.2447, 74167: 4.711, 76511: 2.0484, 76746: 3.4583, 80932: 1.981, 84503: 4.075, 89182: 6.5028, 93204: 6.9083, 93332: 3.0267, 98439: 3.2973, 104251: 0.4753, 107873: 5.992, 110522: 0.7767, 111058: 1.8425, 111889: 6.5028, 111986: 0.9319, 113303: 4.1357, 115245: 0.033, 117298: 5.992, 118195: 5.2988, 120999: 3.7947, 125140: 12.6006, 134386: 2.4309, 134398: 1.4009, 136113: 0.6987, 139343: 5.2988, 140750: 4.7682, 149770: 5.3483, 161186: 6.9083, 166148: 2.2736, 166595: 6.9083, 167933: 1.5611, 168553: 3.0752, 170649: 4.5569, 173301: 11.9839, 173657: 11.5912, 180063: 0.5069, 180393: 5.6555, 191439: 6.7063, 192596: 2.1418, 196531: 0.6241, 210019: 5.522, 227953: 3.2973, 234013: 0.6354, 235875: 4.6057, 237522: 2.3597, 240455: 0.0217, 244632: 2.0292, 250736: 3.9125, 263471: 0.0, 263739: 0.4398, 266790: 17.9759, 267402: 5.992, 289043: 5.6555, 292800: 1.2915, 292818: 0.6867, 293156: 0.1689, 305186: 3.3673, 323297: 0.1613, 323305: 0.011, 323325: 0.3847, 328695: 0.7098, 328727: 5.1165, 332003: 0.1205, 337090: 1.5367, 341252: 2.7504, 344097: 1.7042, 348461: 0.6877, 353945: 5.0365, 354917: 1.852, 362969: 0.946, 371591: 4.6057, 398390: 5.8096, 407662: 9.4763, 412025: 1.4789, 419652: 5.2035, 425558: 4.2341, 426836: 2.5452, 428293: 6.5028, 434753: 0.9961, 439248: 4.5104, 439377: 2.9009, 452519: 0.5741, 454636: 5.6555, 458257: 4.8288, 459181: 2.4196, 464941: 1.3868, 466992: 5.522, 468109: 1.9595, 474814: 0.4874, 478131: 2.6042, 479134: 0.149, 482538: 2.3335, 497106: 3.1355, 499510: 4.7682, 500912: 6.2151, 510088: 6.9083, 514624: 3.3529, 521365: 0.117, 522782: 2.2354, 525105: 3.0063, 542480: 1.1415, 542945: 2.2591, 543646: 2.5325, 545195: 3.4117, 571550: 3.3967, 577138: 1.1765, 579026: 4.0585, 581243: 1.8681, 586717: 2.9101, 588512: 6.9083, 589422: 3.9905, 592866: 5.4042, 603493: 1.2281, 603569: 6.9083, 603917: 4.2341, 606892: 3.8637, 613719: 5.0365, 616772: 6.5028, 618498: 0.249, 622798: 1.0705, 629096: 0.329, 632764: 2.2687, 639284: 2.4309, 646508: 2.8565, 649740: 3.5071, 652355: 0.4193, 657366: 6.9083, 658309: 6.5028, 659387: 4.6057, 664379: 1.5056, 675866: 4.2341, 686247: 4.1357, 697409: 0.6401, 697465: 6.9083, 702676: 4.1674, 715648: 0.008, 719643: 6.5028, 725041: 0.0075, 726425: 3.1471, 732982: 1.3828, 741131: 0.0, 742886: 6.9083, 747580: 6.9083, 749100: 1.072, 752998: 3.0371, 755235: 6.5028, 756241: 1.3968, 756451: 5.1165, 756625: 4.8934, 758738: 4.7682, 759837: 4.1357, 762498: 6.5028, 778603: 1.7293, 779136: 2.159, 779505: 3.3529, 779608: 9.1112, 781335: 4.4659, 787763: 0.5992, 794468: 4.2692, 796794: 1.62, 802840: 3.8402, 803185: 1.0249, 803837: 1.1995, 804908: 1.8976, 809878: 0.2057, 812073: 2.7572, 815906: 1.9118, 815970: 1.6, 818916: 3.1241, 825406: 2.8919, 830045: 3.6502, 832566: 1.6378, 839890: 5.992, 847658: 4.0461, 851715: 1.2145, 852617: 1.852, 860388: 2.3081, 867369: 0.1025, 871768: 6.5028, 878608: 2.2123, 887932: 0.799, 888024: 4.8288, 896592: 2.1461, 897504: 0.022, 898388: 4.8288, 910397: 5.8096, 914281: 3.541, 918582: 5.1682, 920335: 0.3307, 921177: 1.2558, 926664: 2.9863, 928512: 3.9125, 936889: 1.2145, 940260: 2.3335, 941480: 0.1015, 942240: 3.1706, 948895: 0.2842, 951974: 0.2196, 952781: 1.0867, 952784: 2.8829, 956125: 0.2533, 959994: 0.008, 967207: 5.6555, 968718: 5.8096, 969934: 3.427, 972429: 1.0404, 974335: 2.6042, 978439: 4.657, 982207: 6.9083, 985572: 3.1016, 994354: 1.3592, 994604: 5.0365, 1000429: 2.7494, 1006277: 4.4659, 1009967: 3.7947, 1019293: 3.5071, 1021097: 0.5433, 1025930: 6.9083, 1034938: 9.1138, 1036867: 0.5591, 1039863: 2.2448, 1045972: 2.6042}))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make Predictions on Validation set\n",
    "svmLabelsAndPredsVal=rawValidationData.map(lambda p: (p.label, svm.predict(p.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.724770642202\n",
      "0.927272727273\n"
     ]
    }
   ],
   "source": [
    "#True Negatives\n",
    "print svmLabelsAndPredsVal.filter(lambda (a,b):a==0).filter(lambda (a,b):a==b).count()/float(rawValidationData.filter(lambda a:a.label==0).count())\n",
    "#True Positives\n",
    "print svmLabelsAndPredsVal.filter(lambda (a,b):a==1).filter(lambda (a,b):a==b).count()/float(rawValidationData.filter(lambda a:a.label==1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sqlContext=SQLContext(sc)\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "dataDf=sqlContext.createDataFrame(data.zipWithIndex().map(lambda (a,b):(b,a[1],float(a[0]))),['id','text','label'])\n",
    "#trainDf,valDf,testDf = dataDf.randomSplit(weights,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id=0, text=u'the first image in \" final fantasy : the spirits within \" is a computer-animated close-up of a human eye . \\nit\\'s a beautiful piece of work , remarkably detailed and quite convincing . \\nwhen the picture pulls back to reveal the owner of the eye , however , things change . \\nthe young woman has mesmerizing hair , although it hangs too artfully ? even by movie standards ? to be believed . \\nthe facial features are more detailed than any computer-animation seen to date , but the result is more reminiscent of a very well-crafted doll than anything human . \\nshe is pretty , but bland , and not nearly expressive enough to come off like a person . \\nall the characters in \" final fantasy \" are like that . \\nof the core group , the younger white men and women are all athletic , attractive and indistinct , like applicants for a tv reality show . \\nthe black man is taller and burlier , and the aging scholar is bald , with wrinkles and a beard . \\nnone of them appear to be based on individuals ; they all look like the products of general descriptions given a police sketch artist . \\nit gets worse when they talk and move . \\nwhy is the sarcastic voice of steve buscemi , he of the great twisted face and snaggleteeth , coming out of the mouth of some dreary ken doll ? \\nwhy , for every fluid physical gesture , do we also see herky-jerky puppet-style motions ? \\nmore to the point , who decided a full-length computer animated movie featuring \" hyperreal \" ( their term , not mine ) humanoids was a good idea ? \\n \" final fantasy \" is based on a phenomenally popular video game i\\'ve never played , with a story straight out of japanese anime , which more often than not leaves me bored and depressed . \\nif you\\'re a fan of either , please spare me your letters , as i will focus solely on the finished film and not its source materials . \\nwith an expression-challenged cast , \" final fantasy \" mixes turgid action scenes with heaps of mystical shit . \\nthe result is ugly , confusing and boring . \\nnote : the following reveals the basic plot . \\nif you want to have a fighting chance of making any sense of the movie , i suggest you read it . \\nearth is at war with aliens that appear to feed on human souls . \\nmost of our planet is devastated , with humans living in a few protected cities . \\nwhile the bulk of the survivors focus on military strategies , aki ross ( voiced by ming-na ) and her mentor , dr . sid ( donald sutherland ) , believe in a more organic approach . \\nthey operate on the notion ( quoting straight from the press kit ) \" that all life forms have signature spirit waves that can be identified and contained . \\naki and dr . sid collect a series of organic specimens whose spirit signatures combined will form a wave of equal and opposite intensity to the spirit wave of the alien force . \\nthe waves will , in effect , cancel each other out and disarm the foreign contagion . \\nthey have collected six of the eight key spirits needed to complete their wave . \\nthey are on a desperate hunt to find the remaining two spirits before their time runs out . \" \\nare you still with me ? \\nthere\\'s only a little more . \\naki is infected with the alien force . \\ndr . sid has developed a method of confining the contagion and keeping it from killing her , but the defense wall won\\'t hold much longer . \\nalready , the alien is communicating with aki through her dreams . \\naiding aki and dr . sid are the deep eyes , a group of hard-as-nails types that would have felt at home with the troops in \" aliens . \" \\ncapt . gray edwards ( alec baldwin ) heads the task force that consists of a wise guy ( buscemi ) , a tough woman ( peri gilpin ) and a gentle giant ( ving rhames ) . \\nthrowing a monkey wrench into the plans is the requisite dumb ass : in this case general hein ( james woods ) , who wants to use the zeus cannon to bomb the aliens back to the stone age , even if it destroys earth as well . \\nso there you have it . \\nlike most of the anime i\\'ve seen , the plot combines apocalyptic settings , lots of shooting and fuzzy spirituality , all wrapped up in a save-the-earth bow . \\nbut i\\'m bored with apocalyptic settings . \\ni understand why so many live-action films employ them ? they\\'re cheap ? but animated films can show anything , so why wallow in an industrial trash heap ? \\nthe action scenes and shoot-em-ups don\\'t satisfy either . \\nthe humans move oddly and their facial features are so muted that the talented voice cast can\\'t bring them to life ( in fact , their efforts merely emphasize what we\\'re missing ) . \\naki is especially disappointing ; with her lack of expression and flat delivery , she looks and sound like a brunet version of weena , the eloi girl from 1960\\'s \" the time machine . \" \\ndrab color choices and aliens that appear to have been created in jell-o molds sap the pizzazz from the big set pieces . \\nstudents of computer animation may be fascinated with the technology behind \" final fantasy : the spirits within , \" but i found it sub-par across the board . \\n \" futurama \" does more effective battle visuals , the kids in \" south park \" are far more expressive than these mannequins and any old episode of the contemporary version of \" the outer limits \" does better doom and gloom sci-fi . \\nso who needs this ? \\nnot me . \\n', label=0.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights=[.75,.25]\n",
    "seed=45\n",
    "trainDf,valDf = dataDf.randomSplit(weights,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/ml/classification.py:207: UserWarning: weights is deprecated. Use coefficients instead.\n",
      "  warnings.warn(\"weights is deprecated. Use coefficients instead.\")\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and nb.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "ngram = NGram( inputCol=\"words\", outputCol=\"nGrams\")\n",
    "hashingTF = HashingTF(inputCol=ngram.getOutputCol(), outputCol=\"features\")\n",
    "idf = IDF(minDocFreq=3, inputCol=\"features\", outputCol=\"idf\")\n",
    "nb = LogisticRegression()\n",
    "paramGrid = ParamGridBuilder().addGrid( ngram.setN,[1, 2,3]).build()\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, ngram,hashingTF, idf, nb])\n",
    "\n",
    "\n",
    "\n",
    "#model=pipeline.fit(trainDf)\n",
    "\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    evaluator=MulticlassClassificationEvaluator(), \n",
    "                    numFolds=4)\n",
    "\n",
    "cvModel = cv.fit(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#asd=testDf.take(1)[]\n",
    "#m=model\n",
    "#prediction=model.transform(valDf)\n",
    "result = cvModel.transform(valDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prediction_df = result.select(\"text\", \"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selection=result.select(\"id\", \"text\", \"prediction\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CrossValidatorModel.explainParams of CrossValidatorModel_44bdb91268dbe36e8d2a>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7603305785123967"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection.map(lambda a:(a[2],a[3])).filter(lambda (a,b):a==b).count()/float(selection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.255707762557078"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmLabelsAndPredsTrain.filter(lambda (a,b):a==b).count()/float(rawValidationData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asf dsf ds gdsg fg 7-s t db\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def rem(x):\n",
    "    return re.sub('[^A-Za-z0-9|-]+', ' ',x)\n",
    "    #eturn re.sub(r'[^\\\\W\\S]','',x)\n",
    "print rem('asf dsf  ds gdsg fg 7-s t&*%^*&%db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 118.0 failed 1 times, most recent failure: Lost task 0.0 in stage 118.0 (TID 224, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1293, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-33-8061568180ae>\", line 17, in <lambda>\nNameError: global name 'rem' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1293, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-33-8061568180ae>\", line 17, in <lambda>\nNameError: global name 'rem' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-8061568180ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mstopRem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mStopWordsRemover\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#w=data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstopRem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \"\"\"\n\u001b[0;32m    309\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[1;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mStructType\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \"\"\"\n\u001b[1;32m--> 254\u001b[1;33m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1313\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m-> 1315\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1316\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    937\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 939\u001b[1;33m         \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    940\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 118.0 failed 1 times, most recent failure: Lost task 0.0 in stage 118.0 (TID 224, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1293, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-33-8061568180ae>\", line 17, in <lambda>\nNameError: global name 'rem' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1293, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-33-8061568180ae>\", line 17, in <lambda>\nNameError: global name 'rem' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.feature import IDF\n",
    "from pyspark.mllib.classification import LabeledPoint\n",
    "from pyspark.ml.feature import  Tokenizer\n",
    "#from pyspark.ml.feature import IDF\n",
    "#from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.classification import LogisticRegression\n",
    "#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "#from pyspark.ml.tuning import ParamGridBuilder\n",
    "#from pyspark.ml.tuning import CrossValidator\n",
    "ngram = NGram(inputCol=\"words\", outputCol=\"ngrams\",n=2)\n",
    "tok=Tokenizer(inputCol='text',outputCol='word')\n",
    "stopRem=StopWordsRemover(inputCol='word',outputCol='words')\n",
    "#w=data\n",
    "w=sqlContext.createDataFrame(dataDf.map(lambda a:(a[0],rem(a[1]),a[2])),['id','text','label'])\n",
    "w=tok.transform(w)\n",
    "w=stopRem.transform(w)\n",
    "w=ngram.transform(w)\n",
    "hashingTF = HashingTF()\n",
    "hashedtf=hashingTF.transform(w.map(lambda a:a[5]))\n",
    "hashedtf.cache()\n",
    "hashedtfidf = IDF().fit(hashedtf)\n",
    "hashedtfidf = hashedtfidf.transform(hashedtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[40] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LabeledPoint\n",
    "LPtfidf=w.map(lambda a:(a[0],a[2])).join(hashedtfidf.zipWithIndex().map(lambda (a,b):(b,a))).map(lambda(a,b):LabeledPoint(b[0],b[1]))\n",
    "LPtfidf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights=[.75,.25]\n",
    "seed=45\n",
    "trainDf,valDf = dataDf.randomSplit(weights,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainDfLP,valDfLP = LPtfidf.randomSplit(weights,seed)\n",
    "#w.map(lambda a:(a[0],a[2])).join(hashedtfidf.zipWithIndex().map(lambda (a,b):(b,a))).map(lambda(a,b):LabeledPoint(b[0],b[1])).take(1)#.map(lambda (a,b):LabeledPoint(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\", line 690, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"/usr/lib/python2.7/socket.py\", line 224, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a391ad2cb25b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSVMWithSGD\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVMWithSGD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainDfLP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregParam\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminiBatchFraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregType\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/mllib/classification.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, data, iterations, step, regParam, miniBatchFraction, initialWeights, regType, intercept, validateData, convergenceTol)\u001b[0m\n\u001b[0;32m    528\u001b[0m                                  bool(intercept), bool(validateData), float(convergenceTol))\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSVMModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/mllib/regression.py\u001b[0m in \u001b[0;36m_regression_train_wrapper\u001b[1;34m(train_func, modelClass, data, initial_weights)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelClass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m     \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data should be an RDD of LabeledPoint, but got %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1313\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m-> 1315\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1316\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \"\"\"\n\u001b[0;32m   1266\u001b[0m         \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m         \u001b[0mtotalParts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1268\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m    813\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry)\u001b[0m\n\u001b[0;32m    622\u001b[0m          \u001b[0mthe\u001b[0m \u001b[0mPy4J\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m         \"\"\"\n\u001b[1;32m--> 624\u001b[1;33m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    583\u001b[0m         connection = GatewayConnection(\n\u001b[0;32m    584\u001b[0m             self.address, self.port, self.auto_close, self.gateway_property)\n\u001b[1;32m--> 585\u001b[1;33m         \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    695\u001b[0m                 \u001b[1;34m\"server\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "svm = SVMWithSGD.train(data=trainDfLP, iterations=100, step=1.0, regParam=1000, miniBatchFraction=1.0, initialWeights=None, regType='l2', intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\", line 690, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"/usr/lib/python2.7/socket.py\", line 224, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-2a68b0f4708b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalDfLP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcache\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \"\"\"\n\u001b[0;32m    225\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_cached\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStorageLevel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMEMORY_ONLY_SER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mpersist\u001b[1;34m(self, storageLevel)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \"\"\"\n\u001b[0;32m    240\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_cached\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0mjavaStorageLevel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getJavaStorageLevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorageLevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjavaStorageLevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/context.py\u001b[0m in \u001b[0;36m_getJavaStorageLevel\u001b[1;34m(self, storageLevel)\u001b[0m\n\u001b[0;32m    825\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storageLevel must be of type pyspark.StorageLevel\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 827\u001b[1;33m         \u001b[0mnewStorageLevel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStorageLevel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    828\u001b[0m         return newStorageLevel(storageLevel.useDisk,\n\u001b[0;32m    829\u001b[0m                                \u001b[0mstorageLevel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0museMemory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1186\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m             \"\\n\" + proto.END_COMMAND_PART)\n\u001b[0m\u001b[0;32m   1189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSUCCESS_PACKAGE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mJavaPackage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjvm_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry)\u001b[0m\n\u001b[0;32m    622\u001b[0m          \u001b[0mthe\u001b[0m \u001b[0mPy4J\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m         \"\"\"\n\u001b[1;32m--> 624\u001b[1;33m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    583\u001b[0m         connection = GatewayConnection(\n\u001b[0;32m    584\u001b[0m             self.address, self.port, self.auto_close, self.gateway_property)\n\u001b[1;32m--> 585\u001b[1;33m         \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    695\u001b[0m                 \u001b[1;34m\"server\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server"
     ]
    }
   ],
   "source": [
    "pred=valDfLP.map(lambda p: (p.label, svm.predict(p.features)))\n",
    "pred.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\", line 690, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"/usr/lib/python2.7/socket.py\", line 224, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-32999d09b5b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#pred.take(3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#pred.map(lambda a:a[0]).distinct().collect()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalDfLP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#sc.parallelize([1,0,1]).zipWithIndex().map(lambda (a,b):(b,a)).collect()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdistinct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mmapPartitions\u001b[1;34m(self, f, preservesPartitioning)\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitionsWithIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmapPartitionsWithIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mmapPartitionsWithIndex\u001b[1;34m(self, f, preservesPartitioning)\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;36m6\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \"\"\"\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mPipelinedRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmapPartitionsWithSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, prev, func, preservesPartitioning)\u001b[0m\n\u001b[0;32m   2338\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2339\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreservesPartitioning\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2340\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2341\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prev_jrdd_deserializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2342\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_jrdd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2377\u001b[0m         command = (self.func, profiler, self._prev_jrdd_deserializer,\n\u001b[0;32m   2378\u001b[0m                    self._jrdd_deserializer)\n\u001b[1;32m-> 2379\u001b[1;33m         \u001b[0mpickled_cmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbvars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2380\u001b[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(),\n\u001b[0;32m   2381\u001b[0m                                              \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickled_cmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[1;34m(sc, command, obj)\u001b[0m\n\u001b[0;32m   2300\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m<<\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# 1M\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2301\u001b[0m         \u001b[1;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2302\u001b[1;33m         \u001b[0mbroadcast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2303\u001b[0m         \u001b[0mpickled_command\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2304\u001b[0m     \u001b[1;31m# There is a bug in py4j.java_gateway.JavaClass with auto_convert\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/context.py\u001b[0m in \u001b[0;36mbroadcast\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    739\u001b[0m         \u001b[0mbe\u001b[0m \u001b[0msent\u001b[0m \u001b[0mto\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0monly\u001b[0m \u001b[0monce\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m         \"\"\"\n\u001b[1;32m--> 741\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mBroadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pickled_broadcast_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccum_param\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/pyspark/broadcast.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sc, value, pickle_registry, path)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNamedTemporaryFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_temp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jbroadcast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadBroadcastFromFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pickle_registry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_registry\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1186\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m             \"\\n\" + proto.END_COMMAND_PART)\n\u001b[0m\u001b[0;32m   1189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSUCCESS_PACKAGE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mJavaPackage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjvm_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry)\u001b[0m\n\u001b[0;32m    622\u001b[0m          \u001b[0mthe\u001b[0m \u001b[0mPy4J\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m         \"\"\"\n\u001b[1;32m--> 624\u001b[1;33m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    583\u001b[0m         connection = GatewayConnection(\n\u001b[0;32m    584\u001b[0m             self.address, self.port, self.auto_close, self.gateway_property)\n\u001b[1;32m--> 585\u001b[1;33m         \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    695\u001b[0m                 \u001b[1;34m\"server\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server"
     ]
    }
   ],
   "source": [
    "#pred.take(3)\n",
    "#pred.map(lambda a:a[0]).distinct().collect()\n",
    "print pred.filter(lambda (a,b):a==b).count()/float(valDfLP.count())\n",
    "#sc.parallelize([1,0,1]).zipWithIndex().map(lambda (a,b):(b,a)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "documents=data.map(lambda a:a[1].split(' '))\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(documents)\n",
    "from pyspark.mllib.feature import IDF\n",
    "\n",
    "# ... continue from the previous example\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, (1048576,[120,1105,2756,5642,10435,10779,14043,15356,19960,25180,26055,27250,28184,32488,34371,34528,44152,44471,45671,46240,47388,48344,50031,50900,51828,52029,55285,57093,60932,61639,62127,64061,65685,66260,66659,68211,69925,70604,72299,72477,73312,75282,79595,80736,82747,84977,85967,87333,88315,96629,96888,103111,104838,109891,110454,110873,112703,122692,124971,128463,129612,130857,131534,135430,137315,138594,140910,144834,149925,150819,151476,157210,165668,167917,169325,170137,173670,174195,176861,176972,188514,190554,191201,197789,200082,202893,202998,203517,207697,208189,208293,208380,209042,211482,212521,212955,214400,218830,227772,228485,233481,233878,236687,237801,243505,243552,243937,244410,246561,247092,247512,247518,248481,248674,255248,256531,257780,261947,266666,266735,269559,270334,274991,275547,279736,279934,282946,283128,291695,291822,292053,293913,294554,295731,295979,298369,299448,304346,308489,309196,314615,315206,319775,321990,321994,327355,331528,333869,333879,336001,336321,342582,344432,345983,346280,346365,349443,350157,350314,359012,360104,361053,361861,362850,363909,364310,365092,366344,368065,368482,368865,369064,370047,371061,371290,371702,376156,376347,376922,377348,377566,378803,379782,384972,388526,391756,391938,392411,394719,401891,402053,405443,406617,408208,411845,416764,419873,421703,422120,431459,433801,433909,434858,437946,438602,438796,443630,444804,446892,449710,450085,454050,458331,461978,464247,465470,466740,470430,478867,479362,480571,486628,486771,487013,487707,488720,489253,494296,498480,498625,499549,501098,501904,503157,503707,506190,514644,517042,517141,519817,522944,528646,529372,533051,533448,535915,537717,540322,544720,545868,550286,552665,557993,558544,559717,561634,563030,563258,564368,568726,569749,573001,589304,593929,595239,597746,601149,604135,604621,605587,608684,612934,618159,623112,630355,632149,632536,636054,643577,644116,650160,657453,658010,660733,663450,666651,666919,672515,672770,675120,675369,686135,695695,696232,697123,699140,700622,702198,703005,703409,703744,704619,704651,706162,706487,708223,712508,712693,712871,718153,724543,727023,730191,730859,733928,734981,743915,751587,753990,754492,756817,758211,766779,768349,769140,771708,772176,773465,776960,779490,780748,783360,784629,786828,787603,789017,790965,791182,791312,794214,799235,802713,803220,809575,809929,811321,812949,815329,828453,830244,834541,836396,839164,840116,841318,843943,846723,847075,848301,849264,851943,853333,855797,865748,866622,867590,878271,882139,888933,888982,895440,898513,900073,901187,902764,904061,906445,919258,932081,934215,937666,938027,938104,939349,939512,941823,942575,947787,953814,956941,958724,961381,962351,963131,964877,966769,967583,967884,969895,972371,974608,975198,976080,976134,979856,980339,980574,982926,983243,985728,988620,990466,994219,998837,999001,999311,1002688,1006745,1008734,1011135,1012797,1015502,1021211,1021464,1025034,1025456,1027254,1029941,1031397,1036672,1040703,1043202,1043429,1044751,1045074,1048410],[5.99196442215,6.90825515402,6.90825515402,6.50279004592,6.50279004592,6.50279004592,4.51035988123,5.80964286536,6.90825515402,6.50279004592,6.50279004592,6.50279004592,6.50279004592,6.90825515402,6.90825515402,6.62188578687,6.90825515402,6.50279004592,6.90825515402,6.90825515402,6.90825515402,6.90825515402,6.21510797346,6.90825515402,6.90825515402,3.42701506469,6.50279004592,6.90825515402,6.90825515402,5.99196442215,6.50279004592,6.21510797346,6.90825515402,6.50279004592,6.90825515402,6.50279004592,6.90825515402,6.90825515402,6.50279004592,6.90825515402,6.50279004592,6.50279004592,6.50279004592,6.50279004592,6.21510797346,6.50279004592,6.90825515402,6.90825515402,4.60567006103,6.90825515402,6.90825515402,2.95701143544,5.99196442215,6.50279004592,6.90825515402,6.21510797346,5.65549218553,6.90825515402,6.90825515402,6.90825515402,6.50279004592,6.50279004592,6.90825515402,5.99196442215,6.50279004592,6.50279004592,6.90825515402,6.21510797346,6.50279004592,6.90825515402,5.80964286536,6.90825515402,6.21510797346,5.99196442215,6.90825515402,6.50279004592,6.90825515402,6.90825515402,6.90825515402,6.90825515402,6.90825515402,6.50279004592,2.15036388102,6.50279004592,6.90825515402,6.90825515402,6.90825515402,6.90825515402,6.21510797346,6.90825515402,4.96234500497,6.50279004592,11.3109843711,6.50279004592,4.42334850424,6.50279004592,5.65549218553,6.50279004592,6.50279004592,5.99196442215,6.90825515402,6.90825515402,6.90825515402,6.50279004592,6.90825515402,6.90825515402,5.99196442215,6.50279004592,6.50279004592,5.80964286536,6.90825515402,6.90825515402,6.50279004592,6.90825515402,5.5219607929,6.90825515402,6.90825515402,2.15898462406,6.90825515402,6.90825515402,6.90825515402,6.50279004592,6.90825515402,6.21510797346,6.50279004592,6.90825515402,6.90825515402,6.90825515402,6.90825515402,13.816510308,6.90825515402,6.90825515402,6.21510797346,6.50279004592,6.50279004592,6.50279004592,6.90825515402,6.90825515402,6.50279004592,6.90825515402,4.82881361234,6.50279004592,6.90825515402,5.1164956848,6.21510797346,6.50279004592,6.90825515402,6.50279004592,6.90825515402,6.50279004592,6.50279004592,6.90825515402,6.50279004592,6.90825515402,6.90825515402,6.90825515402,4.82881361234,6.21510797346,6.90825515402,6.90825515402,6.90825515402,6.50279004592,6.50279004592,6.21510797346,6.50279004592,6.90825515402,6.90825515402,6.90825515402,6.90825515402,6.90825515402,6.90825515402,6.90825515402,13.0055800918,6.90825515402,6.90825515402,6.90825515402,6.90825515402,6.50279004592,5.80964286536,6.21510797346,6.90825515402,6.50279004592,6.50279004592,6.90825515402,6.90825515402,6.90825515402,5.99196442215,9.65762722469,6.90825515402,6.90825515402,6.21510797346,6.90825515402,6.90825515402,6.90825515402,6.50279004592,6.90825515402,5.80964286536,6.50279004592,5.99196442215,32.5139502296,6.50279004592,6.50279004592,6.90825515402,6.90825515402,6.50279004592,6.90825515402,6.90825515402,6.90825515402,6.90825515402,6.90825515402,6.90825515402,6.90825515402,4.51035988123,6.90825515402,6.90825515402,6.21510797346,4.82881361234,6.90825515402,6.90825515402,6.50279004592,6.21510797346,6.90825515402,6.50279004592,6.90825515402,5.29881724159,6.50279004592,6.50279004592,26.0111601837,6.90825515402,6.90825515402,6.90825515402,6.90825515402,5.5219607929,6.50279004592,6.90825515402,0.928104366816,5.99196442215,6.90825515402,6.50279004592,5.20350706179,6.50279004592,6.50279004592,6.90825515402,6.90825515402,4.2341065046,6.90825515402,6.50279004592,6.21510797346,6.21510797346,5.99196442215,6.21510797346,6.90825515402,6.90825515402,6.90825515402,5.65549218553,6.90825515402,6.90825515402,5.99196442215,6.90825515402,6.50279004592,6.50279004592,6.50279004592,6.90825515402,5.65549218553,6.21510797346,6.50279004592,6.90825515402,6.90825515402,6.50279004592,6.50279004592,6.21510797346,6.21510797346,6.90825515402,6.50279004592,6.50279004592,6.21510797346,6.90825515402,6.90825515402,6.90825515402,6.50279004592,6.50279004592,6.90825515402,6.90825515402,4.96234500497,6.50279004592,6.21510797346,5.99196442215,5.99196442215,6.50279004592,6.50279004592,6.90825515402,6.90825515402,6.21510797346,6.90825515402,4.2341065046,6.90825515402,6.50279004592,6.90825515402,6.90825515402,6.90825515402,6.90825515402,6.90825515402,6.50279004592,13.816510308,6.50279004592,6.90825515402,6.50279004592,4.60567006103,6.90825515402,6.90825515402,6.21510797346,6.50279004592,6.50279004592,6.90825515402,6.21510797346,6.90825515402,6.50279004592,6.90825515402,6.50279004592,6.21510797346,6.50279004592,6.50279004592,6.50279004592,4.65696335542,6.90825515402,6.50279004592,4.10489477312,6.50279004592,6.50279004592,6.90825515402,6.21510797346,5.20350706179,6.50279004592,6.50279004592,6.90825515402,6.21510797346,6.90825515402,6.21510797346,5.1164956848,6.90825515402,6.90825515402,6.90825515402,6.21510797346,6.50279004592,6.90825515402,6.50279004592,6.50279004592,6.90825515402,5.5219607929,6.90825515402,6.50279004592,6.90825515402,6.90825515402,6.50279004592,6.50279004592,6.50279004592,13.0055800918,6.50279004592,6.50279004592,6.90825515402,6.90825515402,5.65549218553,6.21510797346,6.90825515402,6.90825515402,6.90825515402,6.50279004592,6.50279004592,6.90825515402,6.21510797346,6.21510797346,6.50279004592,6.50279004592,6.50279004592,6.90825515402,6.21510797346,6.90825515402,6.21510797346,6.90825515402,6.50279004592,6.21510797346,6.90825515402,6.50279004592,6.21510797346,6.50279004592,5.03645297712,5.29881724159,6.50279004592,5.99196442215,6.90825515402,6.21510797346,4.96234500497,6.90825515402,6.50279004592,4.55687989686,6.90825515402,6.90825515402,6.90825515402,6.90825515402,6.50279004592,6.50279004592,6.90825515402,6.21510797346,6.90825515402,6.21510797346,6.90825515402,6.90825515402,6.50279004592,6.90825515402,6.21510797346,6.90825515402,6.50279004592,6.90825515402,6.50279004592,6.90825515402,6.90825515402,6.90825515402,6.90825515402,6.21510797346,6.50279004592,5.5219607929,6.50279004592,4.96234500497,6.50279004592,6.50279004592,3.61241828802,6.90825515402,3.8637327163,5.65549218553,6.50279004592,6.50279004592,6.21510797346,6.50279004592,6.90825515402,5.80964286536,6.50279004592]))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, ('a', 'c'))]"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print LPtfidf.take(1)\n",
    "aa=sc.parallelize([(1,'a')])\n",
    "bb=sc.parallelize([(1,'c')])\n",
    "aa.leftOuterJoin(bb).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, text=u'the first image in \" final fantasy : the spirits within \" is a computer-animated close-up of a human eye . \\nit\\'s a beautiful piece of work , remarkably detailed and quite convincing . \\nwhen the picture pulls back to reveal the owner of the eye , however , things change . \\nthe young woman has mesmerizing hair , although it hangs too artfully ? even by movie standards ? to be believed . \\nthe facial features are more detailed than any computer-animation seen to date , but the result is more reminiscent of a very well-crafted doll than anything human . \\nshe is pretty , but bland , and not nearly expressive enough to come off like a person . \\nall the characters in \" final fantasy \" are like that . \\nof the core group , the younger white men and women are all athletic , attractive and indistinct , like applicants for a tv reality show . \\nthe black man is taller and burlier , and the aging scholar is bald , with wrinkles and a beard . \\nnone of them appear to be based on individuals ; they all look like the products of general descriptions given a police sketch artist . \\nit gets worse when they talk and move . \\nwhy is the sarcastic voice of steve buscemi , he of the great twisted face and snaggleteeth , coming out of the mouth of some dreary ken doll ? \\nwhy , for every fluid physical gesture , do we also see herky-jerky puppet-style motions ? \\nmore to the point , who decided a full-length computer animated movie featuring \" hyperreal \" ( their term , not mine ) humanoids was a good idea ? \\n \" final fantasy \" is based on a phenomenally popular video game i\\'ve never played , with a story straight out of japanese anime , which more often than not leaves me bored and depressed . \\nif you\\'re a fan of either , please spare me your letters , as i will focus solely on the finished film and not its source materials . \\nwith an expression-challenged cast , \" final fantasy \" mixes turgid action scenes with heaps of mystical shit . \\nthe result is ugly , confusing and boring . \\nnote : the following reveals the basic plot . \\nif you want to have a fighting chance of making any sense of the movie , i suggest you read it . \\nearth is at war with aliens that appear to feed on human souls . \\nmost of our planet is devastated , with humans living in a few protected cities . \\nwhile the bulk of the survivors focus on military strategies , aki ross ( voiced by ming-na ) and her mentor , dr . sid ( donald sutherland ) , believe in a more organic approach . \\nthey operate on the notion ( quoting straight from the press kit ) \" that all life forms have signature spirit waves that can be identified and contained . \\naki and dr . sid collect a series of organic specimens whose spirit signatures combined will form a wave of equal and opposite intensity to the spirit wave of the alien force . \\nthe waves will , in effect , cancel each other out and disarm the foreign contagion . \\nthey have collected six of the eight key spirits needed to complete their wave . \\nthey are on a desperate hunt to find the remaining two spirits before their time runs out . \" \\nare you still with me ? \\nthere\\'s only a little more . \\naki is infected with the alien force . \\ndr . sid has developed a method of confining the contagion and keeping it from killing her , but the defense wall won\\'t hold much longer . \\nalready , the alien is communicating with aki through her dreams . \\naiding aki and dr . sid are the deep eyes , a group of hard-as-nails types that would have felt at home with the troops in \" aliens . \" \\ncapt . gray edwards ( alec baldwin ) heads the task force that consists of a wise guy ( buscemi ) , a tough woman ( peri gilpin ) and a gentle giant ( ving rhames ) . \\nthrowing a monkey wrench into the plans is the requisite dumb ass : in this case general hein ( james woods ) , who wants to use the zeus cannon to bomb the aliens back to the stone age , even if it destroys earth as well . \\nso there you have it . \\nlike most of the anime i\\'ve seen , the plot combines apocalyptic settings , lots of shooting and fuzzy spirituality , all wrapped up in a save-the-earth bow . \\nbut i\\'m bored with apocalyptic settings . \\ni understand why so many live-action films employ them ? they\\'re cheap ? but animated films can show anything , so why wallow in an industrial trash heap ? \\nthe action scenes and shoot-em-ups don\\'t satisfy either . \\nthe humans move oddly and their facial features are so muted that the talented voice cast can\\'t bring them to life ( in fact , their efforts merely emphasize what we\\'re missing ) . \\naki is especially disappointing ; with her lack of expression and flat delivery , she looks and sound like a brunet version of weena , the eloi girl from 1960\\'s \" the time machine . \" \\ndrab color choices and aliens that appear to have been created in jell-o molds sap the pizzazz from the big set pieces . \\nstudents of computer animation may be fascinated with the technology behind \" final fantasy : the spirits within , \" but i found it sub-par across the board . \\n \" futurama \" does more effective battle visuals , the kids in \" south park \" are far more expressive than these mannequins and any old episode of the contemporary version of \" the outer limits \" does better doom and gloom sci-fi . \\nso who needs this ? \\nnot me . \\n', label=0.0)]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok=Tokenizer(inputCol='text',outputCol='words')\n",
    "w=tok.transform(dataDf)\n",
    "from pyspark.ml.feature import NGram\n",
    "ngram = NGram(inputCol=\"words\", outputCol=\"ngrams\",n=2)\n",
    "w=ngram.transform(w)\n",
    "s=rawTrainData.take(1)[0]\n",
    "#s\n",
    "dataDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# The training data folder must be passed as first argument\n",
    "#languages_data_folder = sys.argv[1]\n",
    "dataset = load_files('/home/vagrant/data/mine/txt_sentoken',encoding='utf8',decode_error='replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "#from sklearn.\n",
    "\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
    "\n",
    "pipeline=Pipeline([('vect',TfidfVectorizer(min_df=3,max_df=.95)),('clf',LinearSVC(C=1000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "    # more useful.\n",
    "    # Fit the pipeline on the training set using grid search for the parameters\n",
    "#?GridSearchCV\n",
    "parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    }\n",
    "#print gridSearch.get_params()\n",
    "#?GridSearchCV\n",
    "gridSearch=GridSearchCV(pipeline,parameters,n_jobs=1)\n",
    "gridSearch.fit(docs_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.81533, std: 0.00817, params: {'vect__ngram_range': (1, 1)}, mean: 0.83067, std: 0.01496, params: {'vect__ngram_range': (1, 2)}]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.97      0.99      0.98       251\n",
      "        pos       0.99      0.97      0.98       249\n",
      "\n",
      "avg / total       0.98      0.98      0.98       500\n",
      "\n",
      "[[248   3]\n",
      " [  7 242]]\n"
     ]
    }
   ],
   "source": [
    "# TASK: print the cross-validated scores for the each parameters set\n",
    "# explored by the grid search\n",
    "print(grid_search.grid_scores_)\n",
    "\n",
    "# TASK: Predict the outcome on the testing set and store it in a variable\n",
    "# named y_predicted\n",
    "y_predicted = grid_search.predict(docs_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                        target_names=dataset.target_names))\n",
    "\n",
    "# Print and plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f=open('/home/vagrant/mine/grid','r')\n",
    "grid_search=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, text=u'the first image in \" final fantasy : the spirits within \" is a computer-animated close-up of a human eye . \\nit\\'s a beautiful piece of work , remarkably detailed and quite convincing . \\nwhen the picture pulls back to reveal the owner of the eye , however , things change . \\nthe young woman has mesmerizing hair , although it hangs too artfully ? even by movie standards ? to be believed . \\nthe facial features are more detailed than any computer-animation seen to date , but the result is more reminiscent of a very well-crafted doll than anything human . \\nshe is pretty , but bland , and not nearly expressive enough to come off like a person . \\nall the characters in \" final fantasy \" are like that . \\nof the core group , the younger white men and women are all athletic , attractive and indistinct , like applicants for a tv reality show . \\nthe black man is taller and burlier , and the aging scholar is bald , with wrinkles and a beard . \\nnone of them appear to be based on individuals ; they all look like the products of general descriptions given a police sketch artist . \\nit gets worse when they talk and move . \\nwhy is the sarcastic voice of steve buscemi , he of the great twisted face and snaggleteeth , coming out of the mouth of some dreary ken doll ? \\nwhy , for every fluid physical gesture , do we also see herky-jerky puppet-style motions ? \\nmore to the point , who decided a full-length computer animated movie featuring \" hyperreal \" ( their term , not mine ) humanoids was a good idea ? \\n \" final fantasy \" is based on a phenomenally popular video game i\\'ve never played , with a story straight out of japanese anime , which more often than not leaves me bored and depressed . \\nif you\\'re a fan of either , please spare me your letters , as i will focus solely on the finished film and not its source materials . \\nwith an expression-challenged cast , \" final fantasy \" mixes turgid action scenes with heaps of mystical shit . \\nthe result is ugly , confusing and boring . \\nnote : the following reveals the basic plot . \\nif you want to have a fighting chance of making any sense of the movie , i suggest you read it . \\nearth is at war with aliens that appear to feed on human souls . \\nmost of our planet is devastated , with humans living in a few protected cities . \\nwhile the bulk of the survivors focus on military strategies , aki ross ( voiced by ming-na ) and her mentor , dr . sid ( donald sutherland ) , believe in a more organic approach . \\nthey operate on the notion ( quoting straight from the press kit ) \" that all life forms have signature spirit waves that can be identified and contained . \\naki and dr . sid collect a series of organic specimens whose spirit signatures combined will form a wave of equal and opposite intensity to the spirit wave of the alien force . \\nthe waves will , in effect , cancel each other out and disarm the foreign contagion . \\nthey have collected six of the eight key spirits needed to complete their wave . \\nthey are on a desperate hunt to find the remaining two spirits before their time runs out . \" \\nare you still with me ? \\nthere\\'s only a little more . \\naki is infected with the alien force . \\ndr . sid has developed a method of confining the contagion and keeping it from killing her , but the defense wall won\\'t hold much longer . \\nalready , the alien is communicating with aki through her dreams . \\naiding aki and dr . sid are the deep eyes , a group of hard-as-nails types that would have felt at home with the troops in \" aliens . \" \\ncapt . gray edwards ( alec baldwin ) heads the task force that consists of a wise guy ( buscemi ) , a tough woman ( peri gilpin ) and a gentle giant ( ving rhames ) . \\nthrowing a monkey wrench into the plans is the requisite dumb ass : in this case general hein ( james woods ) , who wants to use the zeus cannon to bomb the aliens back to the stone age , even if it destroys earth as well . \\nso there you have it . \\nlike most of the anime i\\'ve seen , the plot combines apocalyptic settings , lots of shooting and fuzzy spirituality , all wrapped up in a save-the-earth bow . \\nbut i\\'m bored with apocalyptic settings . \\ni understand why so many live-action films employ them ? they\\'re cheap ? but animated films can show anything , so why wallow in an industrial trash heap ? \\nthe action scenes and shoot-em-ups don\\'t satisfy either . \\nthe humans move oddly and their facial features are so muted that the talented voice cast can\\'t bring them to life ( in fact , their efforts merely emphasize what we\\'re missing ) . \\naki is especially disappointing ; with her lack of expression and flat delivery , she looks and sound like a brunet version of weena , the eloi girl from 1960\\'s \" the time machine . \" \\ndrab color choices and aliens that appear to have been created in jell-o molds sap the pizzazz from the big set pieces . \\nstudents of computer animation may be fascinated with the technology behind \" final fantasy : the spirits within , \" but i found it sub-par across the board . \\n \" futurama \" does more effective battle visuals , the kids in \" south park \" are far more expressive than these mannequins and any old episode of the contemporary version of \" the outer limits \" does better doom and gloom sci-fi . \\nso who needs this ? \\nnot me . \\n', label=0.0)]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import \n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and nb.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "ngram = NGram( inputCol=\"words\", outputCol=\"nGrams\")\n",
    "hashingTF = HashingTF(inputCol=ngram.getOutputCol(), outputCol=\"features\")\n",
    "idf = IDF(minDocFreq=3, inputCol=\"features\", outputCol=\"idf\")\n",
    "nb = LogisticRegression()\n",
    "paramGrid = ParamGridBuilder().addGrid( ngram.setN,[1, 2,3]).build()\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, ngram,hashingTF, idf, nb])\n",
    "\n",
    "\n",
    "\n",
    "#model=pipeline.fit(trainDf)\n",
    "\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    evaluator=MulticlassClassificationEvaluator(), \n",
    "                    numFolds=4)\n",
    "\n",
    "cvModel = cv.fit(trainDf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
